{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9140a9b3",
   "metadata": {},
   "source": [
    "# __Project Name__ : __Personalized Research Assistant Using RAG__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d485d6",
   "metadata": {},
   "source": [
    ">> __Problem Description__ : __In this era of AI, Students and Researchers spend lot of their time browsing research paper's meaning and its understanding through various sources.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0bf4e3",
   "metadata": {},
   "source": [
    ">> __Solution__ : __The simple solution could be that a user send their paper and ask questions about it, e.g.,, Key Findings of the paper, Summarization of the paper, etc.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573af814",
   "metadata": {},
   "source": [
    "> ### This file is the testing file for the project, the actual project can be done in .py file i.e., Python file and a Frontend file also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e7ade72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the necessary Imports...\n",
    "\n",
    "# 1. For LLMs (Embeddings and Chats)\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Accessing API keys\n",
    "load_dotenv()\n",
    "\n",
    "hf_api_key = os.getenv(\"HF_API_KEY\")\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_api_key\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint, HuggingFaceEndpointEmbeddings\n",
    "\n",
    "# For prompting \n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# For output parsers\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# for document loading (PDF Docs)\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# For splitting the docs\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# For vectore store and also Retriever\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bea359",
   "metadata": {},
   "source": [
    ">> Step-1 Indexing (Document Ingestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7c900d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader the pdf using loader\n",
    "pdf_loader = PyPDFLoader('F.pdf')\n",
    "docs = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a1a9b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 0, 'page_label': '1'}, page_content='THE FAISS LIBRARY\\nMatthijs Douze\\nFAIR, Meta\\nAlexandr Guzhva\\nZilliz\\nChengqi Deng\\nDeepSeek\\nJeff Johnson\\nFAIR, Meta\\nGergely Szilvasy\\nFAIR, Meta\\nPierre-Emmanuel Mazar´e\\nFAIR, Meta\\nMaria Lomeli\\nFAIR, Meta\\nLucas Hosseini\\nSkip Labs\\nHerv´e J´egou\\nFAIR, Meta\\nAbstract\\nVector databases typically manage large collections of\\nembedding vectors. Currently, AI applications are\\ngrowing rapidly, consequently, the number of embed-\\ndings that need to be stored and indexed is increas-\\ning. The Faiss library is dedicated to vector similarity\\nsearch, a core functionality of vector databases. Faiss\\nis a toolkit of indexing methods and related primi-\\ntives used to search, cluster, compress and transform\\nvectors. This paper describes the trade-offs in vector\\nsearch and the design principles of Faiss in terms of\\nstructure, approach to optimization and interfacing.\\nWe benchmark key features of the library and discuss\\na few selected applications to highlight its broad ap-\\nplicability.\\n1 Introduction\\nThe emergence of deep learning has induced a shift in\\nhow complex data is stored and searched, noticeably\\nby the development of embeddings. Embeddings are\\nvector representations, typically produced by a neu-\\nral network, that map (embed) the input media item\\ninto a vector space, where the locality encodes the se-\\nmantics of the input. Embeddings are extracted from\\nvarious forms of media: words [59, 10], text [24, 40],\\nimages [15, 69], users and items for recommenda-\\ntion [65]. They can even encode object relations, for\\ninstance multi-modal text-image or text-audio rela-\\ntions [31, 70].\\nEmbeddings are employed as an intermediate rep-\\nresentation for further processing, e.g. self-supervised\\nimage embeddings are input to shallow supervised\\nimage classifiers [14, 15]. They are also leveraged as\\na pretext task for self-supervision [18]. In fact, embed-\\ndings are a compact intermediate representation that\\ncan be re-used for several purposes.\\nIn this paper, we consider embeddings used directly\\nto compare media items. The embedding extractor is\\ndesigned so that the distance between embeddings re-\\nflects the similarity between their corresponding me-\\ndia. As a result, conducting neighborhood search in\\nthis vector space offers a direct implementation of\\nsimilarity search between media items.\\nSimilarity search is also popular for tasks where\\nend-to-end learning would not be cost-efficient. For\\nexample, a k-nearest-neighbor classifier is more effi-\\ncient to upgrade with new training samples than a\\nclassification neural net. This explains why the usage\\nof industrial database management systems (DBMS),\\nthat offer a vector storage and search functionality,\\nhas increased in the last years. These DBMS are at\\nthe junction of traditional databases and Approximate\\nNearest Neighbor Search (ANNS) algorithms. Until\\nrecently, the latter were mostly considered for specific\\nuse-cases or in research.\\nFrom a practical perspective, the embedding extrac-\\ntion and the vector search algorithm are bound by an\\n“embedding contract” on the embedding distance:\\n• The embedding extractor, typically a neural net-\\nwork in modern systems, is trained so that dis-\\ntances between embeddings are aligned with the\\ntask to perform.\\n• The vector index performs neighbor search\\namong the embedding vectors as accurately as\\npossible w.r.t. exact search results given the\\nagreed distance metric.\\nFaiss is a library for ANNS. The core library is a\\ncollection of C++ source files without external depen-\\ndencies. Faiss also provides a comprehensive Python\\nwrapper for its C++ core. It is designed to be used\\nboth from simple scripts and as a building block of a\\nDBMS. In contrast with other libraries that focus on a\\nsingle indexing method, Faiss is a toolbox that con-\\ntains a variety of indexing methods that commonly\\ninvolve a chain of components (preprocessing, com-\\npression, non-exhaustive search, etc.). In this paper,\\nwe show that there exists a choice between a dozen\\nindex types, and the optimal one usually depends on\\nthe problem’s constraints.\\nTo summarize what Faiss is not: Faiss does not ex-\\ntract features – it only indexes embeddings that have\\nbeen extracted by a different mechanism; Faiss is not\\na service – it only provides functions that are run as\\npart of the calling process on the local machine; Faiss\\nis not a database – it does not provide concurrent write\\n1\\narXiv:2401.08281v3  [cs.LG]  11 Feb 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 1, 'page_label': '2'}, page_content='access, load balancing, sharding, transaction manage-\\nment or query optimization. The scope of the library\\nis intentionally limited to ANNS algorithmic imple-\\nmentation.\\nThe basic structure of Faiss is anindex that can have\\nmultiple implementations described in this paper. An\\nindex can store a number of database vectors that are\\nprogressively added to it. At search time, a query vec-\\ntor is submitted to the index. The index returns the\\ndatabase vector that is closest to the query vector w.r.t.\\nthe Euclidean distance. There are many variants of\\nthis functionality: instead of just the nearest neighbor,\\nk nearest neighbors are returned; instead of a fixed\\nnumber of neighbors, only the vectors within a certain\\nrange are returned; batches of vectors can be searched\\nin parallel; other metrics besides the Euclidean dis-\\ntance are supported; the search can use either CPUs\\nor GPUs.\\nSince its open-source release in 2017, Faiss has\\nemerged as one of the most popular vector search\\nlibraries, boasting 30k GitHub stars and more than\\n4000 citations of its GPU implementation paper [47].\\nThe Faiss packages have been downloaded 3M times.\\nMajor vector database companies, such as Zilliz and\\nPinecone, either rely on Faiss as their core engine or\\nhave reimplemented Faiss algorithms.\\nThis paper exposes the design principles of Faiss.\\nA similarity search library has to trade off between\\ndifferent constraints (Section 3) using two main tools:\\nvector compression (Section 4) and non-exhaustive\\nsearch (Section 5). We also review a few applica-\\ntions of Faiss for trillion-scale indexing, text retrieval,\\ndata mining, and content moderation (Section 7). The\\nappendix discusses how Faiss is structured and en-\\ngineered to be flexible and usable from other tools.\\nThroughout the paper, we refer to functions or classes\\nin the Faiss codebase 1 as well as the documentation 2\\nusing this specific style.\\n2 Related work\\nIndexing methods. Over the past decade, a steady\\nstream of papers about indexing methods has been\\npublished. Faiss encompasses a broad range of algo-\\nrithms, catering to a diverse spectrum of use cases.\\nOne of the most popular approaches in the indus-\\ntry is to employ Locality Sensitive Hashing (LSH) as a\\nway to compress embeddings into compact codes. In\\nparticular, the cosine sketch [16] produces binary vec-\\ntors such that the Hamming distance is an estimator\\nof the cosine similarity between the original embed-\\ndings. The compactness of these sketches enables stor-\\ning and searching very large databases of media con-\\ntent [54], without the requirement to store the original\\nembeddings. We refer the reader to the early survey\\nby [90] for research on binary codes.\\nSince the work by [44], quantization-based ANN\\nhas emerged as a powerful alternative to binary\\n1https://github.com/facebookresearch/faiss\\n2https://faiss.ai/\\ncodes [89]. We refer the reader to the survey by [58]\\nthat discusses numerous research works related to\\nquantization-based compact codes.\\nLSH often refers to indexing with multiple parti-\\ntions, such as E2LSH [22]. However, we do not con-\\nsider this scenario as the performance is generally in-\\nferior to that of learnt partitions [66]. Early data-aware\\nmethods that proved successful on large datasets in-\\nclude multiple partitions based on kd-tree or hierar-\\nchical k-means [62]. They are often combined with\\ncompressed-domain representation and are especially\\nappropriate for very large-scale settings [43, 44].\\nAfter the introduction of NN-descent [26], graph-\\nbased ANN algorithms have emerged as a viable\\nalternative to space partitioning methods. No-\\ntably, HNSW, currently the most popular indexing\\nmethod [55] for medium-sized dataset, has been im-\\nplemented in HNSWlib.\\nSoftware packages. Most of the research works on\\nvector search have been open-sourced, and some of\\nthese evolved in relatively comprehensive software\\npackages for vector search. For instance, FLANN in-\\ncludes several index types and a distributed imple-\\nmentation described extensively in [62]. The first im-\\nplementation of product quantization relied on the\\nYael library [27], that already had a few of the Faiss\\nprinciples: optimized primitives for clustering meth-\\nods (GMM and k-means), scripting language interface\\n(Matlab and Python) and benchmarking operators.\\nNMSlib, a package originally designed for text re-\\ntrieval, was the first to include HNSW [11]. It also pro-\\nvides several index types. The HNSWlib library later\\nbecame the reference implementation of HNSW [55].\\nGoogle’s SCANN library is a thoroughly optimized\\nimplementation of IVFPQ [44] on SIMD and includes\\nseveral index variants for various database scales.\\nSCANN was open-sourced together with [36], which\\nnotably omits the engineering optimizations that un-\\nderpin the library’s remarkable speed. Diskann [79]\\nis Microsoft’s foundational graph-based vector search\\nlibrary, originally built to leverage hybrid RAM/flash\\nmemory. It also offers a RAM-only version. It was\\nlater extended to perform efficient updates [78], out-\\nof-distribution search [42] and filtered search [34].\\nFaiss was open-sourced concurrently with the pub-\\nlication [47], which details the GPU implementation\\nof several index types. The present paper comple-\\nments this previous work by describing the library as\\na whole.\\nConcurrently, various software libraries from the\\ndatabase world were extended or developed to do\\nvector search. Milvus [88] uses its Knowhere li-\\nbrary, which relies on Faiss as one of its core engines.\\nPinecone [12] initially relied on Faiss, although the en-\\ngine was later rewritten in Rust. Weaviate [86] is a\\ncomposite retrieval engine that includes vector search\\namong other methods.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 2, 'page_label': '3'}, page_content='Theoretical guarantees. Several approximate algo-\\nrithms were inspired by the Johnson-Lindenstrauss\\nlemma, which stipulates that one can embed a set\\nof high-dimensional points into a lower-dimensional\\nspace while almost preserving the distances. For in-\\nstance, under certain assumptions about the data dis-\\ntribution, LSH algorithms based on random partition-\\ning (e.g. projection) provide statistical guarantees for\\nthe range search problem. As this objective is only\\na proxy to the problem of nearest neighbor search,\\nthere are no formal guarantees w.r.t. any metrics such\\nas nearest-neighbor recall. In practice, algorithms\\nbased on random partitioning are significantly out-\\nperformed by data-aware partitioning [66, 2]. Simi-\\nlarly, graph-based algorithms are difficult to compare\\nfrom a theoretical perspective if the data distribution\\nis unknown.\\nTechniques based on vector compression, either\\nwith binary sketches or quantization, often corre-\\nspond to estimators of the target distance. For some\\nof these methods, the variance of these estimators can\\nbe computed in closed form [16, 44], characterizing\\nthe distance estimation error. Again, the quality of\\nthe estimator is only a proxy for the actual k-nearest-\\nneighbor problem.\\nConsidering these factors and the importance of ex-\\necution speed, the quality of ANN search is typically\\nevaluated by experimental comparisons on publicly\\navailable benchmarks.\\nBenchmarks and competitions. The leading\\nbenchmark for million-scale datasets is ANN-\\nbenchmarks [3]. It compares about 50 implemen-\\ntations of ANNS. The big-ANN [76] challenge\\nintroduced large-scale settings, with 6 datasets con-\\ntaining 1 billion vectors each. Faiss was used as a\\nbaseline for the challenge and multiple submissions\\nwere derived from it. The 2023 edition of the chal-\\nlenge is at a smaller scale (10M vectors) but introduces\\nmore complex tasks, including a filtered track where\\nFaiss served as a baseline method [75].\\nDatasets. Early datasets are based on keypoint fea-\\ntures like SIFT [53] used in image matching. We use\\nBIGANN [46], a dataset of 128-dimensional SIFT fea-\\ntures. Later, when global image descriptors produced\\nby neural nets became popular, the Deep1B dataset\\nwas released [6], with 96-dimensional image features\\nextracted with Google LeNet [82]. For this paper, we\\nintroduce a dataset of 768-dimensional Contriever text\\nembeddings [40] that are compared using inner prod-\\nuct similarity. The embeddings are computed with\\nEnglish Wikipedia passages. The higher dimension of\\nthese embeddings is representative of contemporary\\napplications.\\nEach dataset has 10k query vectors and 20M to\\n350M training vectors. We indicate the size of the\\ndatabase explicitly, for example “Deep1M” means the\\ndatabase contains the 1M first vectors of Deep1B. The\\ntraining, database and query vectors are sampled ran-\\nTable 1: Common notations used throughout the paper.\\nNotation Meaning\\nd vector dimension\\nN number of vectors\\nq ∈ Rd query vector\\nxi ∈ Rd ith database vector\\nk number of requested results\\nε ∈ R+ radius for range search\\nK number of centroids for quantization\\nM number of sub-quantizers\\ndomly from the same distribution: we do not address\\nout-of-distribution data [42, 8] in this work.\\n3 Performance axes of a vector\\nsearch library\\nVector search is a well-defined, unambiguous oper-\\nation. In its simplest formulation, given a set of\\ndatabase vectors {xi, i= 1..N} ⊂Rd and a query vec-\\ntor q ∈ Rd, it computes\\nn = argmin\\ni=1..N\\n∥q − xi∥. (1)\\nThe minimum can be computed with a direct algo-\\nrithm by iterating over all database vectors: this is\\nbrute force search . A slightly more general and com-\\nplex operation is to compute the k nearest neighbors\\nof q:\\n(n1, ..., nk, ∗, ...,∗) = argsort\\ni=1..N\\n∥q − xi∥, (2)\\nwhere argsort returns the indices of the array to sort\\nit by increasing distances and ∗ means that an output\\nis ignored. This is what the search method of a Faiss\\nindex returns. A related operation is to find all the\\nelements that are within some ε distance to the query:\\nR = {n = 1..N s.t. ∥q − xn∥ ≤ε}, (3)\\nwhich is computed with the range search method.\\nDistance measures. In the equations above, we\\nleave the definition of the distance undefined. The\\nmost commonly used distances in Faiss are the L2 dis-\\ntance, the cosine similarity and the inner product simi-\\nlarity (for the latter two, theargmin should be replaced\\nwith an argmax). These measures have useful analyt-\\nical properties: for example, they are invariant under\\nd-dimensional rotations.\\nThe Euclidean distance is the default for compar-\\ning vectors [53]. When vectors are obtained by metric\\nlearning, the cosine similarity is often used to avoid\\ncollapsing or exploding embeddings [23]. Maximum\\ninner product search (MIPS) is most often used for rec-\\nommendation systems that require to compare user\\nand item embeddings [65].\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 3, 'page_label': '4'}, page_content='These measures can be made equivalent by pre-\\nprocessing transformations on the query and/or the\\ndatabase vectors. Table 2 summarizes the preprocess-\\ning transformations mapping different measures. To\\nour knowledge, some of these were already identi-\\nfied [7, 37], while others are new.\\nNote that vectors transformed in this manner have\\na very anisotropic distribution [60]: the additional di-\\nmension incurred for many transformations is not ho-\\nmogeneous w.r.t. other dimensions. This can make\\nindexing more difficult, in particular using product or\\nscalar quantization methods. See Section 4.2 for miti-\\ngations.\\n3.1 Brute force search\\nDeveloping an efficient implementation of brute force\\nsearch is not trivial [21, 47]. It requires (1) an efficient\\nway of computing the distances and (2) an efficient\\nway of keeping track of the k smallest distances.\\nComputing distances in Faiss is performed either by\\ndirect distance computations, or, when query vectors\\nare provided in large enough batches, using a matrix\\nmultiplication decomposition [47, Equation 2]. The\\ncorresponding Faiss functions are exposed in knn and\\nknn gpu for CPU and GPU, respectively.\\nCollecting the top- k smallest distances is usually\\ndone via a binary heap on CPU [27, section 2.1] or a\\nsorting network on GPU [47, 63]. For larger values of\\nk, it is more efficient to use a reservoir: an unordered\\nresult buffer of size k′ > kthat is resized to k when it\\noverflows.\\nFaiss’s IndexFlat implements brute force search.\\nHowever, for large datasets this approach becomes\\ntoo slow. In low dimensions, there are branch-and-\\nbound methods that yield exact search results. How-\\never, in large dimensions they provide no speedup\\nover brute force search [91].\\nBesides, for some applications, small variations in\\ndistances are not significant enough to distinguish\\napplication-level positive and negative items [83].\\nIn these cases, approximate nearest neighbor search\\n(ANNS) becomes interesting.\\n3.2 Metrics for Approximate Nearest\\nNeighbor Search\\nWith ANNS, the user accepts imperfect results, which\\nopens the door to a new solution design space. With\\nexact search, the database is represented as a plain ma-\\ntrix. For ANNS, the database may be preprocessed\\ninto an indexing structure, more simply referred to as\\nan index in the following.\\nAccuracy metrics. In ANNS the accuracy3 is mea-\\nsured as a discrepancy with the exact search results\\nfrom (2) and (3). Note that this is an intermediate goal:\\nthe end-to-end accuracy depends on (1) how well the\\n3Metrics involved in a tradeoff are indicated in a specific font.\\ndistance metric correlates with the item matching ob-\\njective and (2) the quality of ANNS, which is what we\\nmeasure here.\\nAccuracy of k-nearest neighbor search is generally\\nevaluated with “ n-recall@k”, which is the fraction of\\nthe n ground-truth nearest neighbors that are in the k\\nfirst search results (with n ≤ k). Most often k = 1or\\nn = k (in which case the measure is a.k.a. “intersection\\nmeasure”). When n = k = 1, the recall measure and in-\\ntersection are the same, and the recall is called “ac-\\ncuracy”. In some publications [44], recall@ k means\\n1-recall@k, while in others [77] it corresponds to k-\\nrecall@k.\\nFor range search, the exact search result is obtained\\nby applying (3), using threshold ε. To yield result\\nlist bR, the approximate search uses a (possibly differ-\\nent) threshold ε′. Thus, standard retrieval metrics can\\nbe computed: precision P = |R ∩ bR|/| bR| and recall\\nR = |R ∩ bR|/|R|. By sweeping ε′ from small to large,\\nthe result list bR increases, producing a precision-recall\\ncurve. The area under the PR-curve is the mean aver-\\nage precision score of range search [76]. Setting ε′ ̸= ε\\nis relevant when approximate vector representations\\ndistort the distance metric (more on this in Section 4).\\nFor vector encoder-decoder pairs, the standard met-\\nric is the mean squared error (MSE) between the origi-\\nnal vector and the reconstructed vector [44, 4, 39]. For\\nan encoder C and a decoder D, the MSE is:\\nMSE =Ex\\n\\x02\\n∥D(C(x)) − x∥2\\n2\\n\\x03\\n. (4)\\nResource metrics. The other axes of the trade-off are\\nrelated to computing resources. During search, the\\nsearch time and memory usage are the main con-\\nstraints. If compression is used, then, the memory us-\\nage can be smaller than what is required to store the\\noriginal vectors.\\nThe index may need to store training data, which in-\\ncurs a constant memory overheadbefore any vector is\\nadded to the index. The index can also add per-vector\\nmemory overhead to the memory used to store each\\nvector. This is the case for graph indexes, that need to\\nstore graph edges for each vector. The memory usage\\nis more complex for settings with hybrid storage such\\nas RAM + flash or GPU memory + RAM.\\nThe index building time is also a resource con-\\nstraint. It may be decomposed into a training time,\\nwhich is independent of the number of vectors added\\nto the index, and the addition time per vector.\\nIn distributed settings or flash-backed storage, the\\nrelevant metric is the number of I/O operations\\n(IOPS), as each read operation fetches a whole page.\\nData layouts that minimize IOPs [79] are more effi-\\ncient than small random accesses. Another possibly\\nlimiting factor is the amount of extra memory needed\\nto pre-compute lookup tables used to speed up search.\\n3.3 Tradeoffs\\nMost often only a subset of metrics matter. For ex-\\nample, when a very large number of searches are per-\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 4, 'page_label': '5'}, page_content='Table 2: One wants to compare a query vector x and a database vector y given a particular metric (rows). The table indicates how to\\npreprocess (x, y) 7−→ (x′, y′) so that an index with another metric (columns) returns the nearest neighbors for the source metric. Some\\ncases require adding 1 or 2 extra dimensions to the original vectors, as is denoted by the vector concatenation symbol [.; .]. The positive\\nscalar parameters α and β are arbitrary and chosen to avoid negative values under a square root.\\nindex metric → L2 IP cos\\nwanted metric ↓\\nL2 identity x′ = [x; −α/2]\\ny′ = [y; ∥y∥2/α]\\nx′ = [x; −α/2; 0]\\ny′ = [βy; β∥y∥2/α;p\\n1 − β2∥y∥2 − β2∥y∥4/α2]\\nIP x′ = [x; 0]\\ny′ = [y;\\np\\nα2 − ∥y∥2] identity x′ = [x; 0]\\ny′ = [αy;\\np\\n1 − ∥αy∥2]\\ncos x′ = x/∥x∥\\ny′ = y/∥y∥\\nx′ = x/∥x∥\\ny′ = y/∥y∥ identity\\n0.15 0.20 0.25 0.30 0.35 0.40\\nAccuracy: 1-recall@1\\n103\\n104\\n105\\n106\\nSpeed: queries per second\\nT ested settings\\nPareto-optimal settings\\nPareto front after 50 experiments\\nFigure 1: Example of exploration of a parameter space with 3\\nparameters (an IndexIVFPQ with polysemous codes and HNSW\\ncoarse quantizer, running on the Deep100M dataset). The total\\nnumber of configurations is 5808, but only 398 experiments are run.\\nWe also show the set of operating points obtained with just 50 ex-\\nperiments.\\nformed on a fixed index, theindex building timedoes\\nnot matter. Or when the number of vectors is so small\\nthat the raw database fits in RAM multiple times, then\\nthe memory usage does not matter. We refer to the\\nmetrics that we care about as the active constraints .\\nNote that accuracy is always an active constraint be-\\ncause if it did not matter, returning random results\\nwould be sufficient (Faiss does actually provide an\\nIndexRandom used in some benchmarking tasks).\\nIn the following sections, we consider that the ac-\\ntive constraints are speed, memory usage and accu-\\nracy. As such, we measure the speed and accuracy of\\nseveral index types and hyperparameter settings un-\\nder a fixed memory budget.\\n3.4 Exploring search-time settings\\nFor a fixed index, there are often one or several search-\\ntime hyperparameters that trade off speed with accu-\\nracy. For example, the nprobe hyperparameter for an\\nIndexIVF, see Section 5. In general, we define hyper-\\nparameters as discrete scalar values such that when\\nthe value is higher, the speed decreases and the ac-\\ncuracy increases. We can then keep only the Pareto-\\noptimal settings, defined as settings that are the fastest\\nfor a given accuracy, or equivalently, that have the\\nhighest accuracy for a given time budget [80].\\nExploring the Pareto-optimal frontier when there is\\na single hyper-parameter consists in sweeping over its\\nvalues with a certain level of granularity, and measur-\\ning the corresponding speed and accuracy.\\nFor multiple hyperparameters, the Pareto frontier\\ncan be recovered by exhaustively testing the Cartesian\\nproduct of these parameters. However, the number of\\nsettings to test grows exponentially with the number\\nof parameters.\\nPruning the parameter space. one can leverage the\\nmonotonous nature of the hyperparameters for effi-\\ncient pruning. We note a tuple of n hyper-parameters\\nπ = (p1, ..., pn) ∈ P= P1 × ... × Pn, and ≤ a partial\\nordering on P: (p1, .., pn) ≤ (p′\\n1, .., p′\\nn) ⇔ ∀i, pi ≤ p′\\ni.\\nLet S(π) and A(π) be the speed and accuracy obtained\\nwith this tuple of parameters. P∗ ⊂ Pis the set of\\nPareto-optimal settings:\\nP∗ =\\n\\x08\\nπ ∈ P|∄π′ ∈ Ps.t. (S(π′), A(π′)) > (S(π), A(π))\\n\\t\\n.\\n(5)\\nSince the individual parameters have a monotonic\\neffect on speed and accuracy, we have\\nπ′ ≥ π ⇒\\n\\x1a S(π′) ≤ S(π),\\nA(π′) ≥ A(π). (6)\\nThus, if a subset bP ⊂ Pof settings is already evalu-\\nated, the following upper bounds hold for a new set-\\nting π ∈ P:\\nS(π) ≤ bS(π) = Inf\\nπ′∈bP s.t. π′≤π\\nS(π′), (7)\\nA(π) ≤ bA(π) = Inf\\nπ′∈bP s.t. π′≥π\\nA(π′). (8)\\nIf any previous evaluation Pareto-dominates these\\nbounds, the setting π does not need to be evaluated:\\n∃π′ ∈ bP s.t. (S(π′), A(π′)) > ( bS(π), bA(π)) ⇒ π /∈ P∗.\\n(9)\\nIn practice, we evaluate settings fromP in a random\\norder. The pruning becomes more and more effective\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 5, 'page_label': '6'}, page_content='throughout the process. It is also more effective when\\nthe number of parameters is larger. Figure 1 shows\\nan example with |P| = 5808combined parameter set-\\ntings. The pruning from (9) reduces this to 398 exper-\\niments, out of which |P∗| = 87are optimal. The Faiss\\nOperatingPoints object implements this.\\n3.5 Refining ( IndexRefine)\\nOne can combine a fast but inaccurate index with a\\nslower, more accurate search. [46, 79, 36]. This is done\\nby querying the fast index to retrieve a shortlist of re-\\nsults. The more accurate search then computes more\\naccurate results only for the shortlist. This requires\\nthe accurate index to allow efficient random access to\\ndatabase vectors. Some implementations use a slower\\nstorage (e.g. flash) for the second index [79, 81].\\nFor the first-level index, the relevant accuracy met-\\nric is the recall at a rank equal to the shortlist size.\\nThus, 1-recall@1000 can be a relevant metric, even if\\nthe end application does not use the 1000th neighbor.\\nSeveral methods based on this refining principle do\\nnot use two separate indexes. Instead, they use two\\nways of interpreting the same compressed vectors: a\\nfast and inaccurate decoding and a slower but more\\naccurate decoding [28, 29, 61, 1, 39] are based on this\\nprinciple. The polysemous codes method [28] is im-\\nplemented in Faiss’s IndexIVFPQ.\\n4 Compression levels\\nFaiss supports various vector codecs: these are meth-\\nods to compress vectors so that they take up less mem-\\nory. A compression methodC : Rd → {1, ..., K}, a.k.a.\\na quantizer, converts a continuous multi-dimensional\\nvector to an integer. This integer is equivalent to a\\nbit string of code size ⌈log2 K⌉. The decoder D :\\n{1, ..., K} →Rd reconstructs an approximation of the\\nvector from the integer. The decoder can only recon-\\nstruct a finite number, K, of distinct vectors.\\nThe search of (1) becomes approximate:\\nn = argmin\\ni=1..N\\n∥q−D(C(xi))∥ = argmin\\ni=1..N\\n∥q−D(Ci)∥, (10)\\nwhere the codes Ci = C(xi) are precomputed and\\nstored in the index. This is the asymmetric distance\\ncomputation (ADC) [44]. The symmetric distance\\ncomputation (SDC) corresponds to the case when the\\nquery vector is also compressed:\\nn = argmin\\ni=1..N\\n∥D(C(q)) − D(Ci)∥. (11)\\nMost Faiss indexes perform ADC as it is more accu-\\nrate: no accuracy is lost on the query vectors. SDC\\nis useful when there is also a storage constraint on\\nthe queries or for indexes where SDC is faster to com-\\npute than ADC. The naive computation of (10) decom-\\npresses the vectors, which has an impact on speed. In\\nmost cases, the distance can be computed in the com-\\npressed domain.\\n4.1 The vector codecs\\nThe k-means vector quantizer ( Kmeans). The ideal\\nvector quantizer minimizes the MSE between the orig-\\ninal and the decompressed vectors. This is formalized\\nin the Lloyd necessary conditions for the optimality of\\na quantizer [52].\\nThe k-means algorithm directly implements these\\nconditions. The K centroids of k-means are an explicit\\nenumeration of all possible vectors that can be recon-\\nstructed.\\nThe k-means vector quantizer is very accurate but\\nthe memory usage and encoding complexity grow\\nexponentially with the code size. Therefore, k-means\\nis impractical to use beyond roughly 3-byte codes, cor-\\nresponding to 16M centroids.\\nScalar quantizers. Scalar quantizers encode each di-\\nmension of a vector independently.\\nA very classical and simple scalar quantizer is LSH\\n(IndexLSH), where each vector component is encoded\\nin a single bit by comparing it to a threshold. The\\nthreshold can be fixed to 0 or trained. Faiss sup-\\nports efficient SDC search of binary vectors via the\\nIndexBinary objects, see Section 4.5.\\nThe ScalarQuantizer also supports uniform quan-\\ntizers that encode a vector component into 8, 6 or 4\\nbits – referred to as SQ8, SQ6, SQ4. A scale and off-\\nset determine which values are reconstructed. They\\ncan be set separately for each dimension on the whole\\nvector. The IndexRowwiseMinMax stores vectors with\\nper-vector normalizing coefficients. Lower-precision\\n16-bit floating point representations are also consid-\\nered as scalar quantizers, SQfp16 and SQbf16.\\nMulti-codebook quantizers. Faiss contains several\\nmulti-codebook quantization (MCQ) options. They\\nare built from M vector quantizers that can recon-\\nstruct K distinct values each. The codes produced\\nby these methods are of the form (c1, ..., cM ) ∈\\n{1, ..., K}M , i.e. each code indexes one of the quantiz-\\ners. The number of reconstructed vectors is KM and\\nthe code size is thus M⌈log2(K)⌉.\\nThe product quantizer ( ProductQuantizer, also\\nnoted PQ) is a simple MCQ that splits the input\\nvector into M sub-vectors and quantizes them sepa-\\nrately [44] with a k-means quantizer. At reconstruc-\\ntion time, the individual reconstructions are concate-\\nnated to produce the final code. In the following, we\\nwill use the notation PQ6x10 for a product quantizer\\nwith 6 sub-vectors each encoded in 10 bits ( M = 6,\\nK = 210).\\nAdditive quantizers are a family of MCQ where the\\nreconstructions from sub-quantizers are summed up\\ntogether. Finding the optimal encoding for a vector\\ngiven the codebooks is NP-hard [4], so, in practice,\\nadditive quantizers use heuristics to find near-optimal\\ncodes.\\nFaiss supports two types of additive quantizers.\\nThe residual quantizer ( ResidualQuantizer) pro-\\nceeds sequentially, by encoding the difference (resid-\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 6, 'page_label': '7'}, page_content='ual) of the vector to encode and the one that is re-\\nconstructed by the previous sub-quantizers [19]. The\\nlocal search quantizer (LocalSearchQuantizer) starts\\nfrom a sub-optimal encoding of the vector and locally\\nexplores neighbording codes in a simulated anneal-\\ning process [56, 57]. We use notations LSQ6x10 and\\nRQ6x10 to refer to additive quantizers with 6 code-\\nbooks of size 210.\\nFaiss also supports a combination of PQ and addi-\\ntive quantizer, ProductResidualQuantizer. In that\\ncase, the vector is split in sub-vectors that are encoded\\nindependently with additive quantizers [5]. The codes\\nfrom the sub-quantizers are concatenated. We use the\\nnotation PRQ2x6x10 to indicate that vectors are split\\nin 2 and encoded independently with RQ6x10, yield-\\ning a total of 12 codebooks of size 210.\\nHierarchy of quantizers. Although this is not by de-\\nsign, there is a strict ordering between the quantizers\\ndescribed before. This means that quantizer i + 1can\\nhave the same set of reproduction values as quantizer\\ni: it is more flexible and more data adaptive. The hier-\\narchy of quantizers is:\\n1. the binary representation with bits +1 and -1 can\\nbe represented as a scalar quantizer with 1 bit per\\ncomponent;\\n2. the scalar quantizer is a product quantizer with\\n1 dimension per sub-vector and uniform per-\\ndimension quantizer;\\n3. the product quantizer is a product-additive quan-\\ntizer where the additive quantizer has a single\\nlevel;\\n4. the product additive quantizer is an additive\\nquantizer where within each codebook all com-\\nponents outside one sub-vector are set to 0 [4];\\n5. the additive quantizer is the general case where\\nthe codebook entries correspond to all possi-\\nble reconstructions obtained by adding elements\\nfrom the subquantizers.\\nThe implications of this hierarchy are (1) the de-\\ngrees of freedom for the reproduction values of quan-\\ntizer i + 1are larger than for i, so it is more accurate\\n(2) quantizer i+1 has a higher capacity so it consumes\\nmore resources in terms of training time and storage\\noverhead than i. In practice, the product quantizer\\noften offers a good trade-off, which explains its wide\\nadoption. The corresponding Faiss Quantizer objects\\nare listed in Appendix A.6.\\n4.2 Vector preprocessing\\nApplying transformations to input vectors before en-\\ncoding can enhance the effectiveness of certain quan-\\ntizers. In particular, d-dimensional rotations are com-\\nmonly used, as they preserve comparison metrics like\\ncosine, L2 and inner product.\\nScalar quantizers assign the same number of bits\\nper vector component. However, for distance com-\\nparisons, if specific vector components have a higher\\nvariance, they have more impact on the distances. In\\nother works, a variable number of bits are assigned\\nper component [71]. However, it is simpler to apply\\na random rotation to the input vectors, which in Faiss\\ncan be done with a RandomRotationMatrix. The ran-\\ndom rotation spreads the variance over all the dimen-\\nsions without changing the measured distances.\\nAn important transform is the Principal Compo-\\nnent Analysis (PCA), that reduces the number of di-\\nmensions d of the input vectors to a user-specified d′.\\nThis operation ( PCAMatrix) is the orthogonal linear\\nmapping that best preserves the variance of the in-\\nput distribution. It is often beneficial to apply PCA to\\nlarge input vectors before quantizing them as k-means\\nquantizers are more likely to “fall” in local minima in\\nhigh-dimensional spaces [51, 45].\\nThe OPQ transformation [33] is a rotation of the in-\\nput space that decorrelates the distribution of each\\nsub-vector of a product quantizer 4. This makes PQ\\nmore accurate in the case where the variance of the\\ndata is concentrated on a few components. The Faiss\\nimplementation OPQMatrix combines OPQ with a di-\\nmensionality reduction. The ITQ transformation [35]\\nsimilarly rotates the input space prior to binarization\\n(ITQMatrix).\\n4.3 Faiss additive quantization options\\nAdditive quantizers exist in two main variants: the\\nresidual quantizer and local search quantizer. They\\nare more complex than most quantizers because the\\nindex building time must be taken into account. In\\nfact, the accuracy of an additive quantizer of a certain\\nsize can always be increased at the cost of an increased\\nencoding time (and training time).\\nAdditive quantizers are based on M codebooks\\nT1, ...TM of size K. The decoding of code C(x) =\\n(c1, ..., cM ) is\\nx′ = D(C(x)) =T1[c1] +... + TM [cM ]. (12)\\nThus, decoding is unambiguous. However, there is\\nno practical way to encode vectors optimally, let alone\\ntrain the codebooks. Enumerating all possible encod-\\nings is of exponential complexity in M.\\nThe residual quantizer (RQ). RQ encodes a vector\\nx sequentially. At stage m, RQ picks the entry that\\nbest reconstructs the residual of x w.r.t. the previous\\nencoding steps:\\ncm = argmin\\nj=1..K\\n\\r\\r\\r\\r\\r\\nm−1X\\ni=1\\nTi[ci] +Tm[j] − x\\n\\r\\r\\r\\r\\r\\n2\\n. (13)\\n4In Faiss terms, OPQ and ITQ are preprocessings. The actual\\nquantization is performed by a subsequent product quantizer or bi-\\nnarization step.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 7, 'page_label': '8'}, page_content='0.16 0.18 0.20 0.22 0.24 0.26 0.28 0.30 0.32\\nMSE\\n101\\n102\\nencoding time (s for 1M vectors, 32 threads)\\n  1\\n 2\\n 4\\n 8\\n 16\\n 32\\nDeep1M encoded to  8 bytes per vector\\n(P)RQ\\n(P)RQ with LUT\\n(P)LSQ\\n(P)LSQ on GPU\\n10x6\\n5x12\\n6x10\\n8x8\\n2x2x12\\n2x3x10\\n2x4x8\\n2x5x6\\n3x2x10\\n0.80 0.85 0.90 0.95 1.00 1.05 1.10\\nMSE\\n102\\n103\\nencoding time (s for 1M vectors, 32 threads)\\n 1\\n 2\\n 4\\n 8\\n 16\\n 32\\nContriever1M encoded to  64 bytes per vector\\n(P)RQ\\n(P)RQ with LUT\\n(P)LSQ\\n(P)LSQ on GPU\\n42x12\\n51x10\\n64x8\\n2x21x12\\n2x25x10\\n2x32x8\\n3x14x12\\n3x17x10\\n4x10x12\\n4x12x10\\nFigure 2: Comparison of additive quantizers in terms of encoding time vs. accuracy (MSE). Lower values are better for both. We consider\\ntwo different regimes: Deep1M (low-dimensional) to 8-bytes codes and Contriever1M (high dimensional) to 64-byte codes. For some RQ\\nvariants, we indicate the beam size setting at which that trade-off was obtained.\\nThis greedy approach tends to get trapped in local\\nminima. As a mitigation, the encoder maintains a\\nbeam of max beam size of possible codes and picks\\nthe best code at stage M. This parameter adjusts the\\ntrade-off between encoding time and accuracy.\\nTo speed up the encoding, the norm of (13) can be\\ndecomposed into the sum of:\\n• ∥Tm[j]∥2 is precomputed and stored;\\n•\\n\\r\\r\\rPm−1\\ni=1 Ti[ci] − x\\n\\r\\r\\r\\n2\\nis the encoding error of the\\nprevious step m − 1;\\n• −2⟨Tm[j], x⟩ is computed on entry to the encod-\\ning (it is the only computation complexity that\\ndepends on d);\\n• 2 Pm−1\\nℓ=1 ⟨Tm[j], Tℓ[cℓ]⟩ is also precomputed.\\nThis decomposition is used when use beam LUT is set.\\nIt is interesting only if d is large and when M is small\\nbecause the storage and compute requirements of the\\nlast term grow quadratically with M.\\nThe local search quantizer (LSQ). At encoding\\ntime, LSQ starts from a suboptimal encoding of the\\nvector and proceeds with a simulated annealing op-\\ntimization to refine the codes. At each optimization\\nstep, LSQ randomly flips codes and then uses Iter-\\nated Conditional Mode (ICM) to optimize the new en-\\ncoding. The number of optimization steps is set with\\nencode ils iters. The LSQ codebooks are trained\\nvia an expectation-maximization procedure (similar\\nto k-means).\\nCompressed-domain search. This consists in com-\\nputing distances without decompressing the stored\\nvectors. It is acceptable to perform pre-computations\\non the query vector q because it is assumed that the\\ncost of these pre-computations will be amortized over\\nmany query-to-code distance comparisons.\\nAdditive quantizer inner products can be computed\\nin the compressed domain:\\n⟨q, x′⟩ =\\nMX\\nm=1\\n⟨Tm[cm], q⟩ =\\nMX\\nm=1\\nLUTm[cm]. (14)\\nThe lookup tables LUTm are computed when a\\nquery vector comes in, similar to product quantizer\\nsearch [44].\\nThis decomposition does not work to compute L2\\ndistances. As a workaround, Faiss uses the decompo-\\nsition [4]\\n∥q − x′∥2 = ∥q∥2 + ∥x′∥2 − 2⟨q, x′⟩. (15)\\nThus, the term ∥x′∥2 must be available at search\\ntime. Using the AdditiveQuantizer.search type\\nconfiguration, it can be appended in the stored\\ncode ( ST norm float32), possibly compressed\\n(ST norm qint8, ST norm qint4,...). It can also be\\ncomputed on the fly (ST norm from LUT) with\\n∥x′∥2 = 2\\nMX\\nm=1\\nm−1X\\nℓ=1\\n⟨Tm[cm], Tℓ[cℓ]⟩ +\\nMX\\nm=1\\n∥Tm[cm]∥2.\\n(16)\\nThere, the norms and dot products are stored in the\\nsame lookup tables as the one used for beam search.\\nTherefore, it trades off search time for memory over-\\nhead to store codes.\\nFigure 2 shows the trade-off between encoding\\ntime and MSE. Given a code size, it is more accu-\\nrate to use a smaller number of sub-quantizers M\\nand a higher K. GPU encoding for LSQ does not\\nhelp systematically. The LUT-based encoding of RQ is\\ninteresing for RQ/PRQ quantization when the beam\\nsize is larger. In the 64-byte regime, we observe that\\nLSQ is not competitive with RQ. PLSQ and PRQ pro-\\ngressively become more competitive for larger mem-\\nory budgets. They are also faster, since they operate\\non smaller vectors.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 8, 'page_label': '9'}, page_content='Deep1M Contriever1M\\n8 16 32 64 128 256\\nbytes per code\\n10 7\\n10 6\\n10 5\\n10 4\\n10 3\\n10 2\\n10 1\\n100\\nMSE\\nLSQ5x12\\nRQ10x12\\nPRQ2x8x12\\nRQ21x12\\nPQ2x14,PQ32x12\\nRQ42x12\\nPRQ16x4x10\\nPRQ32x4x8\\nSQfp16\\nSQ\\nPQ\\nRQ\\nLSQ\\nPLSQ\\nPRQ\\n2-level PQ\\n8 16 32 64 128 256 512\\nbytes per code\\n10 1\\n100\\nMSE\\nLSQ5x12\\nLSQ12x10\\nPRQ2x8x12\\nRQ21x12\\nRQ42x12\\nPRQ2x32x12\\nPRQ2x42x12\\nPQ128x12\\nPRQ32x8x8\\nSQ4\\nSQ\\nPQ\\nRQ\\nLSQ\\nPLSQ\\nPRQ\\n2-level PQ\\nFigure 3: Accuracy vs. code size trade-off for different codecs on the Deep1M and Contriever1M datasets. We show Pareto-optimal\\nvariants with larger dots and indicate the quantizer in text for some of them. Note that contriever vectors can be encoded to MSE=2·10−4\\nin 768 bytes with SQ8 (that setting is widely out-of-range for the plot).\\n4.4 Vector compression benchmark\\nFigure 3 shows the trade-off between code size and\\naccuracy for many variants of the codecs. Addi-\\ntive quantizers are the best options for small code\\nsizes. For larger code sizes, it is beneficial to inde-\\npendently encode several sub-vectors with product-\\nadditive quantizers. LSQ is more accurate than RQ for\\nsmall codes, but does not scale well to longer codes.\\nNote that product quantizers are a bit less accurate\\nthan the additive quantizers but given their low en-\\ncoding time they remains an attractive option. The\\nscalar quantizers perform well for very long codes\\nand are even faster. The 2-level PQ options are what\\nan IVFPQ index uses as encoding: a first-level coarse\\nquantizer and a second level refinement of the resid-\\nual (more about this in Section 5.1).\\n4.5 Binary indexes\\nBinary quantization with symmetric distance compu-\\ntations is a pattern that has been commonly used [90,\\n13]. In this setup, distances are computed in the com-\\npressed domain as Hamming distances. (11) reduces\\nto:\\nn = argmin\\ni=1..N\\n∥C(q) − Ci∥. (17)\\nwhere C(q), Ci ∈ {0, 1}d. Hamming distances are in-\\ntegers in {0..d}. Although they are crude approxima-\\ntions for continuous domain distances, they are fast to\\ncompute, do not require any specific context, and are\\neasy to calibrate in practice.\\nThe IndexBinary indexes support addition and\\nsearch directly from binary vectors. They offer a com-\\npact representation and leverage optimized instruc-\\ntions for distance computations.\\nThe simplest IndexBinaryFlat index performs ex-\\nhaustive search. Three options are offered for non-\\nexhaustive search:\\n• IndexBinaryIVF is a binary counterpart for the\\ninverted-list IndexIVF index described in 5.1.\\n• IndexBinaryHNSW is a binary counterpart for the\\nhierarchical graph-based IndexHNSW index de-\\nscribed in 5.2.\\n• IndexBinaryHash uses prefix vectors as hashes\\nto cluster the database (rather than spheroids as\\nwith inverted lists), and searches only the clusters\\nwith closest prefixes.\\nFinally, theIndexBinaryFromFloat is provided for\\nconvenience. It wraps an arbitrary index and offers a\\nbinary vector interface for its operations.\\n5 Non-exhaustive search\\nNon-exhaustive search is the cornerstone of fast\\nsearch implementations for datasets larger than\\naround N=10k vectors. In that case, the aim of the\\nindexing method is to quickly focus on a subset of\\ndatabase vectors that are most likely to contain the\\nsearch results.\\nA method to do this is Locality Sensitive Hashing\\n(LSH). It amounts to projecting the vectors on a ran-\\ndom direction [22]. The offsets on that direction are\\nthen discretized into buckets where the database vec-\\ntors are stored. At search time, only the nearest buck-\\nets to the query vector’s projection are visited. In prac-\\ntice, several projection directions are needed to make it\\naccurate, at the cost of search time and memory us-\\nage. A fundamental drawback of this method is that\\nit is not data-adaptive, although some improvements\\nare possible [66].\\nAn alternative way of pruning the search space is\\nto use tree-based indexing. In that case, the dataset is\\nstored in the leaves of a tree [62]. When querying a\\nvector, the search starts at the root node. At each in-\\nternal node, the search descends into one of the child\\nnodes depending on a decision rule. The decision rule\\ndepends on how the tree was built: for a KD-tree it\\nis the position w.r.t. a hyperplane, for a hierarchical\\nk-means, it is the proximity to a centroid.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 9, 'page_label': '10'}, page_content='LSH and tree-based methods both aim to extend\\nclassical database search structures to vector search,\\nbecause they have a favorable complexity (constant\\nor logarithmic in N). However, these methods do not\\nscale well for dimensions above 10.\\nFaiss implements two non-exhaustive search ap-\\nproaches that operate at different memory vs. speed\\ntrade-offs: inverted file and graph-based.\\n5.1 Inverted files\\nIVF indexing is a technique that clusters the database\\nvectors at indexing time. This clustering uses a vec-\\ntor quantizer (the coarse quantizer) that outputs KIVF\\ndistinct indices (the nlist field of the IndexIVF ob-\\nject). The coarse quantizer’s KIVF reproduction val-\\nues are called centroids. The vectors of each cluster\\n(possibly compressed) are stored contiguously into in-\\nverted lists, forming an inverted file (IVF). At search\\ntime, only a subset of PIVF clusters are visited (a.k.a.\\nnprobe). The subset is formed by searching the PIVF\\nnearest centroids, as in (2).\\nSetting the number of lists. The KIVF parameter is\\ncentral. In the simplest case, when PIVF is fixed, the\\ncoarse quantizer is exhaustive, the inverted lists con-\\ntain uncompressed vectors, and the inverted lists are\\nall the same size, then the number of distance compu-\\ntations is\\nNdistances = KIVF + PIVF × N/KIVF (18)\\nreaching a minimum when KIVF = √PIVFN. This\\nyields the usual recommendation to set PIVF propor-\\ntional to\\n√\\nN.\\nIn practice, this is just a rough approximation be-\\ncause (1) the PIVF has to increase with the number of\\nlists in order to keep a fixed accuracy (2) the inverted\\nlists sizes are not balanced (3) often the coarse quan-\\ntizer is not exhaustive itself, so the quantization uses\\nfewer than KIVF distance computations, for example\\nit is common to use a non-exhaustive HNSW index to\\nperform the coarse quantization.\\nThe imbalance factor is the relative variance of in-\\nverted list sizes [84]. At search time, if the inverted\\nlists all have the same length, this factor is 1. If they\\nare unbalanced, the expected number of distance com-\\nputations is multiplied by this factor.\\nFigure 4 shows the optimal settings of KIVF for\\nvarious database sizes. For a small KIVF = 4096,\\nthe coarse quantization runtime is negligible and the\\nsearch time increases linearly with the database size.\\nOn larger datasets, it is beneficial to increaseKIVF. As\\nin (18), the ratio KIVF/\\n√\\nN is roughly 15 to 20. Note\\nthat this ratio depends on the data distribution and\\nthe target accuracy. Interestingly, in a regime where\\nKIVF is larger than the optimal setting for N (e.g.\\nKIVF = 218 and N =5M), the PIVF needed to reach\\nthe target accuracy decreases with the dataset size, and\\nso does the search time. This is because when KIVF\\nis fixed and N increases, for a given query vector, the\\n1M 2M 5M 10M 20M 50M\\ndatabase size\\n0.2\\n0.5\\n1\\n2\\n5\\nsearch time (s for 10k queries, 64 threads)\\n 4.1\\n 5.8\\n 14.7\\n 14.7\\n 20.7\\n 20.7\\n 14.7\\n 29.3\\n 18.5\\n 37.1\\ntarget recall 0.9\\nKIVF = 4096\\nKIVF = 8192\\nKIVF = 16384\\nKIVF = 32768\\nKIVF = 65536\\nKIVF = 131072\\nKIVF = 262144\\nscaling model\\nFigure 4: Search time as a function of the database size N for Bi-\\ngANN1B with different KIVF settings. The PIVF is set so that the\\n1-recall@1 is 90%. The full lines indicate that the coarse quantizer is\\nexact, the dashed lines rely on a HNSW coarse quantizer. For some\\nsetting we indicate the ratio KIVF/\\n√\\nN\\nnearest database vector is either the same or a new\\none that is closer, so it is more likely to be found in a\\nquantization cluster nearer to the query.\\nWith a faster non-exhaustive coarse quantizer (e.g.\\nHNSW) it is even more useful to increase KIVF for\\nlarger databases, as the coarse quantization becomes\\nrelatively cheap. At the limit, when KIVF = N,\\nthen all the work is done by the coarse quantizer. In\\nthis scenario, the limiting factor becomes the memory\\noverhead of the coarse quantizer.\\nBy fitting a model of the form t = t0Nα to the tim-\\nings of the fastest index in Figure 4, we can derive a\\nscaling rule for the IVF indexes:\\ntarget recall@1 0.5 0.75 0.9 0.99\\npower α 0.29 0.30 0.34 0.45\\nThus, with this model, the search time increases faster\\nfor higher accuracy targets, but α < 0.5, so the run-\\ntime dependence on the database size is below\\n√\\nN.\\nEncoding residuals. In general, it is more accurate\\nto compress the residuals of the database vectors w.r.t.\\nthe centroids [44, Eq. (28)]. This is either because\\nthe norm of the residuals is lower than that of the\\noriginal vectors, or because residual encoding is a\\nway to take into account a-priori information from the\\ncoarse quantizer. In Faiss, this is controlled via the\\nIndexIVF.by residual flag, which is set to true by de-\\nfault.\\nFigure 5 shows that encoding residuals is beneficial\\nfor shorter codes. For larger codes, the contribution\\nof the residual is less important. Indeed, as the orig-\\ninal data is 96-dimensional, it can be compressed to\\n64 bytes relatively accurately. Note that using higher\\nKIVF also improves the accuracy of the quantizer with\\nresidual encoding. From a pure encoding point of\\nview, the additional bits of information brought by the\\ncoarse quantizer (log2(KIVF) = 10or 14) improve the\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 10, 'page_label': '11'}, page_content='PQ4\\nPQ6\\nPQ8\\nPQ8x10\\nPQ12\\nPQ12x10\\nPQ16x10\\nPQ24\\nPQ24x10\\nPQ32x10\\nPQ48\\nPQ48x10\\nPQ48x12\\n4 8 16 32 64 128\\ncode size (bytes)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8recall @ 1 for ndis < 3000\\nKIVF = 1024, direct encoding\\nKIVF = 1024, encode residuals\\nKIVF = 16384, direct encoding\\nKIVF = 16384, encode residuals\\nFigure 5: Comparing IVF indexes with and without residual encod-\\ning for KIVF ∈ {210, 214} on the Deep1M dataset ( d=96 dimen-\\nsions), with different product quantization settings. We measure\\nthe recall that can be achieved within 3000 distance comparisons.\\naccuracy more when used in this residual encoding\\nthan if they would added to increase the size of a PQ.\\nSpherical clustering for inner product search. Ef-\\nficient indexing for maximum inner product search\\n(MIPS) faces multiple issues: the distribution of query\\nvectors is often different from the database vector\\ndistribution, most notably in recommendation sys-\\ntems [65]; the MIPS datasets are diverse, an algorithm\\nthat obtains a good performance on some dataset will\\nperform badly on another. Besides, [60] show that us-\\ning the preprocessing formulas in Section 3 is a sub-\\noptimal way of indexing for MIPS.\\nSeveral specialized clustering and indexing meth-\\nods were developed for MIPS [36, 60]. Instead,\\nFaiss implements a modification of k-means cluster-\\ning, spherical k-means [25], which normalizes the IVF\\ncentroids at each iteration. One of the MIPS issues is\\ndue to database vectors of very different norms (when\\nthey are normalized, MIPS is equivalent to L2 search).\\nHigh-norm centroids “attract” the database vectors in\\ntheir clusters, which increases the imbalance factor.\\nSpherical k-means is designed to avoid this issue.\\nFigure 6 shows that for the Contriever MIPS dataset,\\nthe imbalance factor is high. It is reduced by using IP\\nassignment instead of L2, and even more with spheri-\\ncal k-means.\\nBig batch search. A common use case for ANNS\\nis search with very large query batches. This ap-\\npears for applications such as large-scale data dedu-\\nplication. In this case, rather than loading an entire\\nindex in memory and processing queries one small\\nbatch at a time, it can be more memory-efficient to\\nload only the quantizer, quantize the queries, and then\\niterate over the index by loading it one chunk at a\\ntime. Big-batch search is implemented in the module\\ncontrib.big batch search.\\n0.4 0.5 0.6 0.7 0.8 0.9 1.0\\n1-recall@1\\n102\\n103\\n104\\nQPS\\nKIVF = 1024, default: imbalance=2.23\\nKIVF = 16384, default: imbalance=3.13\\nKIVF = 1024, IP assignment: imbalance=1.16\\nKIVF = 16384, IP assignment: imbalance=2.17\\nKIVF = 1024, spherical: imbalance=1.12\\nKIVF = 16384, spherical: imbalance=1.39\\nFigure 6: Precision vs. speed trade-off for the MIPS contriever1M\\ndataset. The compared settings are whether the coarse quantizer as-\\nsignement is done using L2 distance (default) or IP assignment and\\nwhether the k-means clustering does a normalization at each iter-\\nation (spherical, IP and L2 assignment are equivalent in that case).\\nThe imbalance factors are indicated for each setting.\\n5.2 Graph based\\nGraph-based indexing consists in building a directed\\ngraph whose nodes are the vectors to index. At search\\ntime, the graph is explored by following the edges to-\\nwards the nodes that are closest to the query vector. In\\npractice, the search is not greedy but maintains a pri-\\nority queue with the most promising edges to explore.\\nThus, the trade-off at search time is given by the num-\\nber of exploration steps: higher is more accurate but\\nslower.\\nA graph-based algorithm is a general framework\\nthat can encompass many variants. In particular, tree-\\nbased search or IVF can be seen as special cases of\\ngraphs. One can see graphs as a way to precompute\\nneighbors for the database vectors, then match the\\nquery to one of the vertices and follow the neighbors\\nfrom there. However, they can also be built to handle\\nout-of-distribution queries [42, 17].\\nGiven this search algorithm, relying on a pure k-\\nnearest neighbor graph is not optimal because neigh-\\nbors are redundant. Therefore, the graph building\\nheuristic consists in balancing edges to nearest neigh-\\nbors and edges that reach more distant nodes. Most\\ngraph methods fix the number of outgoing edges\\nper node, which adjusts the trade-off between search\\nspeed and memory usage . The memory usage per\\nvector breaks down into (1) the possibly compressed\\nvector and (2) the outgoing edges for that vector [29].\\nFaiss implements two graph-based algorithms:\\nHNSW and NSG, respectively in the IndexHNSW and\\nIndexNSG classes.\\nHNSW. The hierarchical navigable small world\\ngraph [55] is a search structure where some randomly\\nselected vertices are promoted to be hubs that are ex-\\nplored first. A notable advantage of HNSW is its abil-\\nity to add vectors on-the-fly.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 11, 'page_label': '12'}, page_content='0.70 0.75 0.80 0.85 0.90 0.95 1.00\\n1-recall@1\\n104\\n105\\n106\\nQueries per second\\n64 \\n128 \\n256 \\n512 \\n1024 \\n64 \\n128 \\n256 \\n512 \\n1024 \\n2048 \\n8 edges\\n16 edges\\n32 edges\\n64 edges\\nFigure 7: Comparison of graph-based indexing methods HNSW\\n(full lines) and NSG (dashes) to index Deep1M. We sweep the trade-\\noffs between speed and accuracy by varying the number of graph\\ntraversal steps (indicated for some of the curves).\\nNSG. The Navigating Spreading-out Graph [32] is\\nbuilt from a k-nearest neighbor graph that must be\\nprovided on input. At building time, some short-\\nrange edges are replaced with longer-range edges.\\nThe input k-nn graph can be built with a brute force\\nalgorithm or with a specialized method such as NN-\\ndescent [26] ( NNDescent). Unlike HNSW, NSG does\\nnot rely on multi-layer graph structures, but uses long\\nconnections to achieve fast navigation. In addition,\\nNSG starts from a fixed center point when searching.\\nDiscussion. Figure 7 compares the speed-accuracy\\ntrade-off for the NSG and HNSW indexes. Their main\\nbuild-time hyperparameter is the number of edges\\nper node, so we tested several settings (for HNSW\\nthis is the number of edges on the base level of the\\nhierarchical graph). The main search-time param-\\neter is the number of graph traversal steps during\\nsearch (parameter efSearch for HNSW and search L\\nfor NSG), which we vary to plot each curve. Increas-\\ning the number of edges improves the results until 64\\nedges, beyond which performance deteriorates. NSG\\nobtains better trade-offs in general, at the cost of a\\nlonger build time . Building the k-NN graph with\\nNN-descent for 1M vectors takes 37 s, and about the\\nsame time with exact, brute force search on a GPU.\\nThe NSG graph is frozen after the first batch of vec-\\ntors is added, there is no easy way to add more vectors\\nafterwards.\\n5.3 How to choose an index\\nIn most cases, choosing an appropriate index can\\nbe done by following a process delineated in Ap-\\npendix A.5. First, one has to decide whether indexing\\nis needed at all: indeed, in some cases a direct brute\\nforce search is the best option. Otherwise, the choice\\nis between IVF and graph-based indexes.\\n0.5 0.6 0.7 0.8 0.9 0.95 0.99 0.999 1\\n10-recall@10\\n103\\n104\\n105\\nQPS (48 threads, batched)\\ndeep-image-96-angular (10M vectors)\\nfaiss\\nscann\\nFigure 8: Comparison between the Faiss and SCANN libraries in-\\ndexing Deep10M in the ann-benchmarks setup (batch mode).\\nIVF vs. graph-based. Graph-based indices are a\\ngood option for indexes where there is no constraint\\non memory usage , typically for indexes below 1M\\nvectors. Beyond 10M vectors, the construction time\\ntypically becomes the limiting factor. For larger in-\\ndexes, where compression is required to even fit the\\ndatabase vectors in memory, IVF indexes are the only\\noption.\\nThe decision tree of Figure 10 provides intial direc-\\ntions. The Faiss wiki 5 features comprehensive bench-\\nmarks for various database sizes and memory bud-\\ngets. To refine the index parameters, benchmarking\\nshould be used.\\n5.4 Benchmarking indexes\\nFaiss includes a benchmarking framework (bench fw)\\nthat optimizes index types and parameters to explore\\naccuracy, memory usage and search time operating\\npoints. The benchmark generates candidate index\\nconfigurations to evaluate, sweeps both construction-\\ntime and search-time parameters, and measures these\\nmetrics.\\nDecoupling encoding and non-exhaustive search op-\\ntions. Beyond a certain scale, search time is deter-\\nmined by the number of distance computations per-\\nformed between the query vector and database vec-\\ntors.\\nAs shown in Sections 5 and 4, Faiss indexes are\\nbuilt as a combination of pruning and compression,\\nsee Table 3. To evaluate index configurations effi-\\nciently, the benchmarking framework takes advantage\\nof this compositional design. The training of vec-\\ntor transformations and k-means clustering for IVF\\ncoarse quantizers are factored out and reused when\\npossible. Coarse quantizers and IVF indices are first\\ntrained and evaluated separately, the parameter space\\nis pruned as described in the previous section, and\\n5https://github.com/facebookresearch/faiss/\\nwiki/Indexing-1G-vectors\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 12, 'page_label': '13'}, page_content='only the combinations of Pareto-optimal components\\nare benchmarked together.\\n5.5 Comparison with other libraries\\nANN-benchmarks [3] is a codebase that compares\\nseveral ANNS implementations. We use this setup\\nto compare Faiss with SCANN [36], another industry-\\nstandard package for vector search. We run the search\\nin the following setting: batch search on the Deep10M\\ndataset with 10-recall@10 as the metric, where the\\ntraining is performed on the database vectors. This\\ndiffers slightly from the evaluation protocol used orig-\\ninally for this dataset.\\nFigure 8 shows that Faiss is 1.5 × to 4× faster than\\nSCANN, depending on the operating point. For Faiss\\nwe used IVF with a Product Residual Quantizer opti-\\nmized for SIMD (see Appendix A.3), followed by re-\\nranking (Section 3.5).\\n6 Database operations\\nIn the experiments above, the indexes are built in one\\ngo with all the vectors, while search operations are\\nperformed with one batch containing all query vec-\\ntors. In real settings, the index evolves over time, vec-\\ntors may be dynamically added or removed, searches\\nmay have to take into account metadata, etc. In this\\nsection we show how Faiss supports some of these\\noperations, mainly on IVF indexes. Specific APIs\\nare available to interface with external storage (Ap-\\npendix A.4) if fine-grained control is required.\\n6.1 Identifier-based operations\\nFaiss indexes support two types of identifiers: sequen-\\ntial and arbitrary ids. Sequential ids are based on\\nthe order of additions in the index. Alternatively, the\\nuser can provide arbitrary 63-bit integer ids associ-\\nated to each vector. The corresponding addition meth-\\nods for the index are add and add with ids. In ad-\\ndition, Faiss supports removing vectors ( remove ids)\\nand updating them ( update vectors) by passing the\\ncorresponding ids.\\nUnlike e.g. Usearch [87], Faiss does not store ar-\\nbitrary metadata with the vectors, only 63-bit integer\\nids can be used (the sign bit is reserved for invalid re-\\nsults).\\nFlat indexes. Sequential indexes ( IndexFlatCodes)\\nstore vectors as a flat array. They support only se-\\nquential ids. When arbitrary ids are needed, the index\\ncan be embedded in a IndexIDMap, that translates se-\\nquence numbers to arbitrary ids using a int64 array.\\nThis enables add with ids and returns the arbitrary\\nids at search time. For more id-based operations, a\\nmore complete wrapper,IndexIDMap2, maps arbitrary\\nids back to sequential ids using a hash table.\\nAs graph indexes rely on an embedded\\nIndexFlatCodes to store the actual vectors, they\\nshould also be wrapped with the mapping classes.\\nHowever, HNSW does not support suppression and\\nmutation, and NSG does not even support adding\\nvectors incrementally. Supporting this requires\\nheuristics to rebuild the graph when it is mutated,\\nwhich are implemented in HNSWlib and [78] but are\\nsuboptimal indexing-wise.\\nIVF indexes. The IVF indexing structure supports\\nuser-provided ids natively at addition and search\\ntime. However, id-based access may require a sequen-\\ntial scan, as the entries are stored in an arbitrary or-\\nder in the inverted lists. Therefore, the IVF index can\\noptionally maintain a DirectMap, which maps user-\\nvisible ids to the inverted list and the offset they are\\nstored in. It supports lookup, removal and update by\\nids. The map can be an array, which is appropriate for\\nsequential ids, or a hash table, for arbitrary 63-bit ids.\\nObviously, the direct map incurs a memory overhead\\nand an add-time computation overhead, therefore, it\\nis disabled by default.\\n6.2 Filtered search\\nVector filtering consists in returning only database\\nvectors based on some search-time criterion, other\\nvectors are ignored. Faiss has basic support for vector\\nfiltering: the user can provide a predicate (IDSelector\\ncallback), and if the predicate returns false on the vec-\\ntor id, the vector is ignored.\\nTherefore, if metadata is needed to filter the vec-\\ntors, the callback function needs to do an indirection\\nto the metadata table, which is inefficient. Another\\napproach is to exploit the unused bits of the identi-\\nfier. If N documents are indexed with sequential ids,\\n63 − ⌈log2(N)⌉ bits are unused.\\nThis is sufficient to store enumerated types (e.g.\\ncountry codes, music genres, license types, etc.), dates\\n(as days since some origin), version numbers, etc.\\nHowever, it is insufficient for more complex metadata.\\nIn the example use case below, we use the available\\nbits to implement more complex filtering.\\nFiltering with bag-of-word vectors. In the filtered\\nsearch track of the BigANN 2023 competition [75],\\neach query and database vector is associated with a\\nfew terms from a fixed vocabulary of size v (for the\\nqueries there are only 1 or 2 words). The filtering con-\\nsists in considering only the database vectors that in-\\nclude all the query terms. . This metadata is given as\\na sparse matrix Mmeta ∈ {0, 1}N×v.\\nThe basic implementation of the filter starts from\\nquery vector q and the associated words w1, w2 ∈\\n{1...v}. Before computing a distance to a vector with\\nid i, it fetches row i of Mmeta to verify that w1 and\\nw2 are in it. This predicate is slow because (1) it re-\\nquires to accessMmeta, which causes cache misses and\\n(2) it performs an iterative binary search in the sparse\\nmatrix structure. Since the callback is called in the\\ntightest inner loop of the search function, and since\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 13, 'page_label': '14'}, page_content='the IVF search tends to perform many vector compar-\\nisons, this has non negligible performance impact.\\nTo speed up the predicate, we can use bit manip-\\nulations. In this example, N = 107, so we use only\\n⌈log2 N⌉ = 24bits of the ids, leaving 63 − 24 = 39bits\\nthat are always 0. We associate to each word j a 39-\\nbit signature S[j], and to each set of words the binary\\n“or” of these signatures. The query is represented by\\nsq = S[w1] ∨ S[w2]. Database entry i with words Wi\\nis represented by si = ∨w∈Wi S[w]. Then the following\\nimplication holds: if {w1, w2} ⊂Wi then all 1 bits of\\nsq are also set to 1 in si:\\n{w1, w2} ⊂Wi ⇒ ¬si ∧ sq = 0, (19)\\nwhich is equivalent to:\\n¬si ∧ sq ̸= 0⇒ {w1, w2} ̸⊂Wi. (20)\\nThis binary test costs only a few machine instructions\\non data that is already in machine registers. It can\\nthus be used as a pre-filter before applying the predi-\\ncate computation. This is implemented in the module\\nbow id selector6.\\nThe remaining degree of freedom is how to choose\\nthe binary signatures, because this rule’s filtering abil-\\nity depends on the choice of the signatures S. We ex-\\nperimented with i.i.d. Bernoulli bits with varying p:\\nthe best setting avoids running the full predicate more\\nthan 4/5 times.\\np= probability of 1 0.05 0.1 0.2 0.5\\nFilter hit rate 75.4% 82.1% 76.4% 42.0%\\nVector-first or metadata-first search. There are two\\npossible approaches to filtered search: vector-first,\\nwhich is described above, and metadata-first, where\\nonly vectors with appropriate metadata are consid-\\nered in vector search. The metadata-first filtering gen-\\nerates a subset of vectors to compare with that can\\nthen be compared using brute force search. Brute\\nforce search is slow but acceptable if the subset size\\nis small, and the results are exact.\\nTherefore, in the context of the BigANN competi-\\ntion [75], the decision to use vector-first or metadata-\\nfirst depends on how large the subset is. To this end,\\nwe map each word w to the list of items that contain\\nw, of size Lw.\\nIf there is a single query word {w1} then the sub-\\nset size is directly accessible as S = Lw1 . With two\\nquery words {w1, w2}, finding the subset size requires\\nintersecting the inverted lists for w1 and w2, which is\\nslow. Instead, one can estimate the size of the subset\\nusing the empirical probability of each word to appear\\nP(w) = Lw/N. Assuming independent draws, the\\nprobability of both words to appear is P(w1)P(w2).\\nTherefore, the expected size of the intersection is S ≈\\nNP (w1)P(w2) = Lw1 Lw2 /N. If S/N < 3 × 10−4\\n(an empirical threshold), then the subset is sufficiently\\n6https://github.com/harsha-simhadri/\\nbig-ann-benchmarks/tree/main/neurips23/filter/\\nfaiss\\nsmall that metadata-first can be applied. Of course, if\\nmetadata-first is selected, the actual intersection has\\nto be computed. Most participants to the competition\\nused similar heuristics.\\n7 Faiss applications\\nFaiss is widely used across the industry, with nu-\\nmerous applications leveraging its capabilities. The\\nfollowing examples highlight notable use cases that\\ndemonstrate exceptional scalability or significant im-\\npact.\\n7.1 Trillion scale index\\nIn this example, we index 1.5 trillion vectors in 144 di-\\nmensions. The indexing needs to be accurate, there-\\nfore the compression of the vectors is limited to 54\\nbytes with a PCA to 72 dimensions and 6-bit scalar\\nquantizer (PCAR72,SQ6).\\nA HNSW coarse quantizer with 10M centroids is\\nused for the IndexIVFScalarQuantizer, trained with\\na simple distributed GPU k-means (implemented in\\nfaiss.clustering).\\nOnce the training is completed, the index is built in\\nthe following three phases:\\n1. shard over ids: add the input vectors in 2000\\nshards independently, producing 2000 indexes\\n(each one fits in 256 GB RAM);\\n2. shard over lists: build the 100 indexes corre-\\nsponding each to a subset of 100k inverted lists.\\nThis is done on 100 different machines, each read-\\ning from the 2000 sharded indices, and writing\\nthe results directly to a distributed file system;\\n3. load the shards: memory-map all 100 indexes on\\na central machine as 100OnDiskInvertedLists (a\\nmemory map of 83 TiB).\\nSteps 1 and 2 are organized to be performed as in-\\ndependent cluster jobs on a few hundred servers (64\\ncores, 256G RAM). Otherwise, the code is written in\\nstandard Faiss in Python.\\nThe central machine that handles searches performs\\nthe coarse quantization and loads the inverted lists\\nfrom the distributed disk partition. The limiting fac-\\ntor is the network bandwidth of this central machine.\\nTherefore, it is more efficient to distribute the search\\non 20 intermediate servers to spread the load. This\\nbrings the search time down to roughly 1 s per query.\\n7.2 Text retrieval\\nFaiss is commonly used for knowledge intensive nat-\\nural language processing tasks. In particular, ANNS is\\nrelevant for information retrieval [85, 67], with appli-\\ncations such as fact checking, entity linking, slot filling\\nor open-domain question answering: these often rely\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 14, 'page_label': '15'}, page_content='on retrieving relevant content across a large-scale cor-\\npus. To that end, embedding models have been opti-\\nmized for text retrieval [40, 49].\\nFinally, [41], [50], [74] and [48] consist of language\\nmodels that have been trained to integrate textual re-\\ntrieval in order to improve their accuracy, factuality or\\ncompute efficiency.\\n7.3 Data mining\\nAnother recurrent application of ANNS and Faiss is in\\nthe mining and curation of large datasets. In particu-\\nlar, Faiss has been used to mine bilingual texts across\\nvery large text datasets retrieved from the web [72, 9],\\nor to organize a language model’s training corpus in\\norder to group together series of documents covering\\nsimilar topics [73].\\nIn the image domain, [64] leverages Faiss to remove\\nduplicates from a dataset containing 1.3B images. It\\nthen relies on efficient indexing in order to mine a cu-\\nrated dataset whose distribution matches the distribu-\\ntion of a target dataset.\\n7.4 Content Moderation\\nOne of the major applications of Faiss is the detection\\nand remediation of harmful content at scale. Human-\\nlabeled examples of policies violating images and\\nvideos are embedded with models such as SSCD [69]\\nand stored in a Faiss index. To decide if a new image\\nor video would violate some policies, a multi-stage\\nclassification pipeline first embeds the content and\\nsearches the Faiss index for similar labeled examples,\\ntypically utilizing range queries. The results are ag-\\ngregated and processed through additional machine\\nclassification or human verification. Since the impact\\nof mistakes is high, good representations should dis-\\ncriminate perceptually similar and different content,\\nand accurate similarity search is required even at bil-\\nlion to trillion scale. The former problem motivated\\nthe Image and Video Similarity Challenges [30, 68].\\n8 Conclusion\\nThroughout the years, Faiss continuously expanded\\nits focus to include the most relevant vector indexing\\ntechniques from research. We plan to continue doing\\nthis to include novel quantization techniques [39], bet-\\nter hardware support for some indexes [63] and new\\nindexing forms, such as associative vector memories\\nfor transformer architectures [20, 92].\\nReferences\\n[1] Kenza Amara, Matthijs Douze, Alexandre\\nSablayrolles, and Herv ´e J ´egou. Nearest neigh-\\nbor search with compact codes: A decoder\\nperspective. In ICMR, 2022.\\n[2] Alexandr Andoni and Ilya Razenshteyn. Op-\\ntimal data-dependent hashing for approximate\\nnear neighbors. In Proceedings of the forty-seventh\\nannual ACM symposium on Theory of computing ,\\npages 793–801, 2015.\\n[3] Martin Aum ¨uller, Erik Bernhardsson, and\\nAlexander Faithfull. Ann-benchmarks: A bench-\\nmarking tool for approximate nearest neighbor\\nalgorithms. Information Systems, 87:101374, 2020.\\n[4] Artem Babenko and Victor Lempitsky. Additive\\nquantization for extreme vector compression. In\\nConference on Computer Vision and Pattern Recogni-\\ntion, 2014.\\n[5] Artem Babenko and Victor Lempitsky. Tree\\nquantization for large-scale similarity search and\\nclassification. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, pages\\n4240–4248, 2015.\\n[6] Artem Babenko and Victor Lempitsky. Efficient\\nindexing of billion-scale datasets of deep descrip-\\ntors. In Conference on Computer Vision and Pattern\\nRecognition, 2016.\\n[7] Yoram Bachrach, Yehuda Finkelstein, Ran Gilad-\\nBachrach, Liran Katzir, Noam Koenigstein, Nir\\nNice, and Ulrich Paquet. Speeding up the xbox\\nrecommender system using a euclidean transfor-\\nmation for inner-product spaces. InProceedings of\\nthe 8th ACM Conference on Recommender systems ,\\npages 257–264, 2014.\\n[8] Dmitry Baranchuk, Matthijs Douze, Yash Upad-\\nhyay, and I Zeki Yalniz. Dedrift: Robust simi-\\nlarity search under content drift. In Proceedings of\\nthe IEEE/CVF International Conference on Computer\\nVision, pages 11026–11035, 2023.\\n[9] Lo ¨ıc Barrault, Yu-An Chung, Mariano Cora\\nMeglioli, David Dale, Ning Dong, Paul-\\nAmbroise Duquenne, Hady Elsahar, Hongyu\\nGong, Kevin Heffernan, John Hoffman, et al.\\nSeamlessm4t-massively multilingual & mul-\\ntimodal machine translation. arXiv preprint\\narXiv:2308.11596, 2023.\\n[10] Piotr Bojanowski, Edouard Grave, Armand\\nJoulin, and Tomas Mikolov. Enriching word vec-\\ntors with subword information.Transactions of the\\nassociation for computational linguistics , 5:135–146,\\n2017.\\n[11] Leonid Boytsov, David Novak, Yury Malkov, and\\nEric Nyberg. Off the beaten path: Let’s replace\\nterm-based retrieval with k-nn search. In Pro-\\nceedings of the 25th ACM international on conference\\non information and knowledge management , pages\\n1099–1108, 2016.\\n[12] Sebastian Bruch, Franco Maria Nardini, Amir\\nIngber, and Edo Liberty. An approximate al-\\ngorithm for maximum inner product search\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 15, 'page_label': '16'}, page_content='over streaming sparse vectors. arXiv preprint\\narXiv:2301.10622, 2023.\\n[13] Zhangjie Cao, Mingsheng Long, Jianmin Wang,\\nand Philip S. Yu. Hashnet: Deep learning to hash\\nby continuation. In Proceedings of the IEEE Inter-\\nnational Conference on Computer Vision (ICCV), Oct\\n2017.\\n[14] Mathilde Caron, Piotr Bojanowski, Armand\\nJoulin, and Matthijs Douze. Deep clustering for\\nunsupervised learning of visual features. In Pro-\\nceedings of the European conference on computer vi-\\nsion (ECCV), pages 132–149, 2018.\\n[15] Mathilde Caron, Hugo Touvron, Ishan Misra,\\nHerv´e J ´egou, Julien Mairal, Piotr Bojanowski,\\nand Armand Joulin. Emerging properties in self-\\nsupervised vision transformers. In Proceedings of\\nthe IEEE/CVF international conference on computer\\nvision, pages 9650–9660, 2021.\\n[16] Moses Charikar. Similarity estimation techniques\\nfrom rounding algorithms. In Proc. ACM symp.\\nTheory of computing, 2002.\\n[17] Meng Chen, Kai Zhang, Zhenying He, Yinan\\nJing, and X Sean Wang. Roargraph: A projected\\nbipartite graph for efficient cross-modal approx-\\nimate nearest neighbor search. arXiv preprint\\narXiv:2408.08933, 2024.\\n[18] Ting Chen, Simon Kornblith, Mohammad\\nNorouzi, and Geoffrey Hinton. A simple\\nframework for contrastive learning of visual\\nrepresentations. In Proceedings of the 37th\\nInternational Conference on Machine Learning ,\\nProceedings of Machine Learning Research,\\npages 1597–1607. PMLR, 2020.\\n[19] Yongjian Chen, Tao Guan, and Cheng Wang. Ap-\\nproximate nearest neighbor search by residual\\nvector quantization. Sensors, 10(12):11259–11273,\\n2010.\\n[20] Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye,\\nYang Zhou, Jianyu Zhang, Niklas Nolte, Yuan-\\ndong Tian, Matthijs Douze, Leon Bottou, Zhihao\\nJia, et al. Magicpig: Lsh sampling for efficient llm\\ngeneration. arXiv preprint arXiv:2410.16179, 2024.\\n[21] Felix Chern, Blake Hechtman, Andy Davis, Ruiqi\\nGuo, David Majnemer, and Sanjiv Kumar. Tpu-\\nknn: K nearest neighbor search at peak flop/s.\\nAdvances in Neural Information Processing Systems,\\n35:15489–15501, 2022.\\n[22] Mayur Datar, Nicole Immorlica, Piotr Indyk, and\\nVahab S Mirrokni. Locality-sensitive hashing\\nscheme based on p-stable distributions. In Pro-\\nceedings of the twentieth annual symposium on Com-\\nputational geometry, pages 253–262, 2004.\\n[23] Jiankang Deng, Jia Guo, Niannan Xue, and Ste-\\nfanos Zafeiriou. Arcface: Additive angular mar-\\ngin loss for deep face recognition. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR), June 2019.\\n[24] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. Bert: Pre-training of deep\\nbidirectional transformers for language under-\\nstanding. arXiv preprint arXiv:1810.04805, 2018.\\n[25] Inderjit S Dhillon and Dharmendra S Modha.\\nConcept decompositions for large sparse text\\ndata using clustering. Machine learning, 42:143–\\n175, 2001.\\n[26] Wei Dong, Charikar Moses, and Kai Li. Effi-\\ncient k-nearest neighbor graph construction for\\ngeneric similarity measures. In Proceedings of the\\n20th international conference on World wide web ,\\npages 577–586, 2011.\\n[27] Matthijs Douze and Herv ´e J ´egou. The yael li-\\nbrary. In Proceedings of the 22nd ACM international\\nconference on Multimedia, pages 687–690, 2014.\\n[28] Matthijs Douze, Herv ´e J ´egou, and Florent Per-\\nronnin. Polysemous codes. In European Confer-\\nence on Computer Vision, 2016.\\n[29] Matthijs Douze, Alexandre Sablayrolles, and\\nHerv´e J´egou. Link and code: Fast indexing with\\ngraphs and compact regression codes. InProceed-\\nings of the IEEE conference on computer vision and\\npattern recognition, pages 3646–3654, 2018.\\n[30] Matthijs Douze, Giorgos Tolias, Ed Pizzi, Zo ¨e\\nPapakipos, Lowik Chanussot, Filip Radenovic,\\nTomas Jenicek, Maxim Maximov, Laura Leal-\\nTaix´e, Ismail Elezi, et al. The 2021 image sim-\\nilarity dataset and challenge. arXiv preprint\\narXiv:2106.09672, 2021.\\n[31] Paul-Ambroise Duquenne, Holger Schwenk, and\\nBenoˆıt Sagot. Sentence-level multimodal and\\nlanguage-agnostic representations. arXiv preprint\\narXiv:2308.11466, 2023.\\n[32] Cong Fu, Chao Xiang, Changxu Wang, and Deng\\nCai. Fast approximate nearest neighbor search\\nwith the navigating spreading-out graph. arXiv\\npreprint arXiv:1707.00143, 2017.\\n[33] Tiezheng Ge, Kaiming He, Qifa Ke, and Jian\\nSun. Optimized product quantization for ap-\\nproximate nearest neighbor search. In Conference\\non Computer Vision and Pattern Recognition, 2013.\\n[34] Siddharth Gollapudi, Neel Karia, Varun\\nSivashankar, Ravishankar Krishnaswamy, Nikit\\nBegwani, Swapnil Raz, Yiyong Lin, Yin Zhang,\\nNeelam Mahapatro, Premkumar Srinivasan,\\nAmit Singh, and Harsha Vardhan Simhadri.\\nFiltered-diskann: Graph algorithms for approx-\\nimate nearest neighbor search with filters. In\\nProceedings of the ACM Web Conference 2023, 2023.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 16, 'page_label': '17'}, page_content='[35] Yunchao Gong, Svetlana Lazebnik, Albert Gordo,\\nand Florent Perronnin. Iterative quantization: A\\nprocrustean approach to learning binary codes\\nfor large-scale image retrieval. IEEE Trans. Pat-\\ntern Analysis and Machine Intelligence, 2012.\\n[36] Ruiqi Guo, Philip Sun, Erik Lindgren, Quan\\nGeng, David Simcha, Felix Chern, and Sanjiv\\nKumar. Accelerating large-scale inference with\\nanisotropic vector quantization. In International\\nConference on Machine Learning. PMLR, 2020.\\n[37] Weixiang Hong, Xueyan Tang, Jingjing Meng,\\nand Junsong Yuan. Asymmetric mapping quanti-\\nzation for nearest neighbor search. IEEE Transac-\\ntions on Pattern Analysis and Machine Intelligence ,\\n42(7):1783–1790, 2019.\\n[38] Jui-Ting Huang, Ashish Sharma, Shuying Sun,\\nLi Xia, David Zhang, Philip Pronin, Janani\\nPadmanabhan, Giuseppe Ottaviano, and Linjun\\nYang. Embedding-based retrieval in facebook\\nsearch. In Proceedings of the 26th ACM SIGKDD\\nInternational Conference on Knowledge Discovery &\\nData Mining, pages 2553–2561, 2020.\\n[39] Iris A.M. Huijben, Matthijs Douze, Matthew J.\\nMuckley, Ruud J.G. van Sloun, and Jakob Ver-\\nbeek. Residual quantization with implicit neural\\ncodebooks. In International Conference on Machine\\nLearning (ICML), 2024.\\n[40] Gautier Izacard, Mathilde Caron, Lucas Hos-\\nseini, Sebastian Riedel, Piotr Bojanowski, Ar-\\nmand Joulin, and Edouard Grave. Unsu-\\npervised dense information retrieval with con-\\ntrastive learning, 2021.\\n[41] Gautier Izacard, Patrick Lewis, Maria Lomeli,\\nLucas Hosseini, Fabio Petroni, Timo Schick, Jane\\nDwivedi-Yu, Armand Joulin, Sebastian Riedel,\\nand Edouard Grave. Atlas: Few-shot learn-\\ning with retrieval augmented language models.\\nJournal of Machine Learning Research, 24(251):1–43,\\n2023.\\n[42] Shikhar Jaiswal, Ravishankar Krishnaswamy,\\nAnkit Garg, Harsha Vardhan Simhadri, and\\nSheshansh Agrawal. Ood-diskann: Efficient\\nand scalable graph anns for out-of-distribution\\nqueries, 2022.\\n[43] Herve Jegou, Matthijs Douze, and Cordelia\\nSchmid. Hamming embedding and weak geo-\\nmetric consistency for large scale image search.\\nIn Computer Vision–ECCV 2008: 10th European\\nConference on Computer Vision, Marseille, France,\\nOctober 12-18, 2008, Proceedings, Part I 10 , pages\\n304–317. Springer, 2008.\\n[44] Herv ´e J ´egou, Matthijs Douze, and Cordelia\\nSchmid. Product quantization for nearest neigh-\\nbor search. IEEE Trans. Pattern Analysis and Ma-\\nchine Intelligence, 2010.\\n[45] Herv ´e J´egou, Florent Perronnin, Matthijs Douze,\\nJorge S ´anchez, Patrick P ´erez, and Cordelia\\nSchmid. Aggregating local image descriptors\\ninto compact codes. IEEE transactions on pattern\\nanalysis and machine intelligence , 34(9):1704–1716,\\n2011.\\n[46] Herv ´e J´egou, Romain Tavenard, Matthijs Douze,\\nand Laurent Amsaleg. Searching in one billion\\nvectors: re-rank with source coding. In Interna-\\ntional Conference on Acoustics, Speech, and Signal\\nProcessing, 2011.\\n[47] Jeff Johnson, Matthijs Douze, and Herv ´e J ´egou.\\nBillion-scale similarity search with GPUs. IEEE\\nTrans. on Big Data, 2019.\\n[48] Urvashi Khandelwal, Omer Levy, Dan Jurafsky,\\nLuke Zettlemoyer, and Mike Lewis. General-\\nization through memorization: Nearest neighbor\\nlanguage models, 2020.\\n[49] Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas\\nOguz, Jimmy Lin, Yashar Mehdad, Wen tau Yih,\\nand Xilun Chen. How to train your dragon: Di-\\nverse augmentation towards generalizable dense\\nretrieval, 2023.\\n[50] Xi Victoria Lin, Xilun Chen, Mingda Chen, Wei-\\njia Shi, Maria Lomeli, Rich James, Pedro Ro-\\ndriguez, Jacob Kahn, Gergely Szilvasy, Mike\\nLewis, Luke Zettlemoyer, and Scott Yih. Ra-\\ndit: Retrieval-augmented dual instruction tun-\\ning. ArXiv, abs/2310.01352, 2023.\\n[51] Shicong Liu, Hongtao Lu, and Junru Shao.\\nImproved residual vector quantization for\\nhigh-dimensional approximate nearest neighbor\\nsearch. arXiv preprint arXiv:1509.05195, 2015.\\n[52] Stuart Lloyd. Least squares quantization in PCM.\\nIEEE Transactions on Information Theory, 1982.\\n[53] David G Lowe. Distinctive image features from\\nscale-invariant keypoints. International Journal of\\nComputer Vision, 60(2), 2004.\\n[54] Qin Lv, Moses Charikar, and Kai Li. Image sim-\\nilarity search with compact data structures. In\\nProceedings of the thirteenth ACM international con-\\nference on Information and knowledge management ,\\npages 208–217, 2004.\\n[55] Yu A Malkov and Dmitry A Yashunin. Efficient\\nand robust approximate nearest neighbor search\\nusing hierarchical navigable small world graphs.\\nIEEE transactions on pattern analysis and machine\\nintelligence, 42(4):824–836, 2018.\\n[56] Julieta Martinez, Joris Clement, Holger H Hoos,\\nand James J Little. Revisiting additive quantiza-\\ntion. In European Conference on Computer Vision ,\\n2016.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 17, 'page_label': '18'}, page_content='[57] Julieta Martinez, Shobhit Zakhmi, Holger H\\nHoos, and James J Little. LSQ++: lower running\\ntime and higher recall in multi-codebook quanti-\\nzation. In European Conference on Computer Vision,\\n2018.\\n[58] Yusuke Matsui, Yusuke Uchida, Herv ´e J ´egou,\\nand Shin’ichi Satoh. A survey of product quan-\\ntization. ITE Transactions on Media Technology and\\nApplications, 2018.\\n[59] Tomas Mikolov, Kai Chen, Greg Corrado, and\\nJeffrey Dean. Efficient estimation of word rep-\\nresentations in vector space. arXiv preprint\\narXiv:1301.3781, 2013.\\n[60] Stanislav Morozov and Artem Babenko. Non-\\nmetric similarity graphs for maximum inner\\nproduct search. Advances in Neural Information\\nProcessing Systems, 31, 2018.\\n[61] Stanislav Morozov and Artem Babenko. Un-\\nsupervised neural quantization for compressed-\\ndomain similarity search. In International Confer-\\nence on Computer Vision, 8 2019.\\n[62] Marius Muja and David G Lowe. Scalable nearest\\nneighbor algorithms for high dimensional data.\\nIEEE transactions on pattern analysis and machine\\nintelligence, 36(11):2227–2240, 2014.\\n[63] Hiroyuki Ootomo, Akira Naruse, Corey Nolet,\\nRay Wang, Tamas Feher, and Yong Wang. Cagra:\\nHighly parallel graph construction and approxi-\\nmate nearest neighbor search for gpus, 2023.\\n[64] Maxime Oquab, Timoth ´ee Darcet, Theo\\nMoutakanni, Huy V . Vo, Marc Szafraniec,\\nVasil Khalidov, Pierre Fernandez, Daniel Haziza,\\nFrancisco Massa, Alaaeldin El-Nouby, Russell\\nHowes, Po-Yao Huang, Hu Xu, Vasu Sharma,\\nShang-Wen Li, Wojciech Galuba, Mike Rabbat,\\nMido Assran, Nicolas Ballas, Gabriel Synnaeve,\\nIshan Misra, Herve Jegou, Julien Mairal, Patrick\\nLabatut, Armand Joulin, and Piotr Bojanowski.\\nDINOv2: Learning robust visual features\\nwithout supervision, 2023.\\n[65] Arkadiusz Paterek. Improving regularized sin-\\ngular value decomposition for collaborative fil-\\ntering. In Proceedings of KDD cup and workshop ,\\n2007.\\n[66] Lo ¨ıc Paulev ´e, Herv ´e J ´egou, and Laurent Amsa-\\nleg. Locality sensitive hashing: A comparison of\\nhash function types and querying mechanisms.\\nPattern recognition letters, 31(11):1348–1358, 2010.\\n[67] Fabio Petroni, Aleksandra Piktus, Angela\\nFan, Patrick Lewis, Majid Yazdani, Nicola\\nDe Cao, James Thorne, Yacine Jernite, Vladimir\\nKarpukhin, Jean Maillard, Vassilis Plachouras,\\nTim Rockt¨aschel, and Sebastian Riedel. KILT: a\\nbenchmark for knowledge intensive language\\ntasks. In Proceedings of the 2021 Conference of\\nthe North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, Online, June 2021. Association for\\nComputational Linguistics.\\n[68] Ed Pizzi, Giorgos Kordopatis-Zilos, Hiral Patel,\\nGheorghe Postelnicu, Sugosh Nagavara Ravin-\\ndra, Akshay Gupta, Symeon Papadopoulos,\\nGiorgos Tolias, and Matthijs Douze. The 2023\\nvideo similarity dataset and challenge. arXiv\\npreprint arXiv:2306.09489, 2023.\\n[69] Ed Pizzi, Sreya Dutta Roy, Sugosh Nagavara\\nRavindra, Priya Goyal, and Matthijs Douze. A\\nself-supervised descriptor for image copy detec-\\ntion. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, pages\\n14532–14542, 2022.\\n[70] Alec Radford, Jong Wook Kim, Chris Hallacy,\\nAditya Ramesh, Gabriel Goh, Sandhini Agarwal,\\nGirish Sastry, Amanda Askell, Pamela Mishkin,\\nJack Clark, et al. Learning transferable visual\\nmodels from natural language supervision. In\\nInternational conference on machine learning , pages\\n8748–8763. PMLR, 2021.\\n[71] Harsimrat Sandhawalia and Herv ´e J ´egou.\\nSearching with expectations. In International\\nConference on Acoustics, Speech, and Signal Process-\\ning, 2010.\\n[72] Holger Schwenk and Matthijs Douze. Learning\\njoint multilingual sentence representations with\\nneural machine translation. In Proceedings of the\\n2nd Workshop on Representation Learning for NLP ,\\npages 157–167, Vancouver, Canada, August 2017.\\nAssociation for Computational Linguistics.\\n[73] Weijia Shi, Sewon Min, Maria Lomeli, Chunting\\nZhou, Margaret Li, Victoria Lin, Noah A. Smith,\\nLuke Zettlemoyer, Scott Yih, and Mike Lewis. In-\\ncontext pretraining: Language modeling beyond\\ndocument boundaries. ArXiv, abs/2310.10638,\\n2023.\\n[74] Weijia Shi, Sewon Min, Michihiro Yasunaga,\\nMinjoon Seo, Rich James, Mike Lewis, Luke\\nZettlemoyer, and Wen tau Yih. Replug: Retrieval-\\naugmented black-box language models, 2023.\\n[75] Harsha Vardhan Simhadri, Martin Aum ¨uller,\\nAmir Ingber, Matthijs Douze, George Williams,\\nMagdalen Dobson Manohar, Dmitry Baranchuk,\\nEdo Liberty, Frank Liu, Ben Landrum, et al. Re-\\nsults of the big ann: Neurips’23 competition.\\narXiv preprint arXiv:2409.17424, 2024.\\n[76] Harsha Vardhan Simhadri, George Williams,\\nMartin Aum ¨uller, Matthijs Douze, Artem\\nBabenko, Dmitry Baranchuk, Qi Chen, Lucas\\nHosseini, Ravishankar Krishnaswamny, Gopal\\nSrinivasa, et al. Results of the neurips’21\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 18, 'page_label': '19'}, page_content='challenge on billion-scale approximate nearest\\nneighbor search. In NeurIPS 2021 Competitions\\nand Demonstrations Track, pages 177–189. PMLR,\\n2022.\\n[77] Harsha Vardhan Simhadri, George Williams,\\nMartin Aum ¨uller, Matthijs Douze, Artem\\nBabenko, Dmitry Baranchuk, Qi Chen, Lucas\\nHosseini, Ravishankar Krishnaswamny, Gopal\\nSrinivasa, Suhas Jayaram Subramanya, and Jing-\\ndong Wang. Results of the neurips’21 challenge\\non billion-scale approximate nearest neighbor\\nsearch. In Proceedings of the NeurIPS 2021 Com-\\npetitions and Demonstrations Track, volume 176 of\\nProceedings of Machine Learning Research , pages\\n177–189. PMLR, 06–14 Dec 2022.\\n[78] Aditi Singh, Suhas Jayaram Subramanya, Rav-\\nishankar Krishnaswamy, and Harsha Vardhan\\nSimhadri. Freshdiskann: A fast and accurate\\ngraph-based ann index for streaming similarity\\nsearch, 2021.\\n[79] Suhas Jayaram Subramanya, Rohan Kadekodi,\\nRavishankar Krishaswamy, and Harsha Vardhan\\nSimhadri. Diskann: Fast accurate billion-point\\nnearest neighbor search on a single node. In\\nNeurips, 2019.\\n[80] Philip Sun, Ruiqi Guo, and Sanjiv Kumar. Au-\\ntomating nearest neighbor search configuration\\nwith constrained optimization. arXiv preprint\\narXiv:2301.01702, 2023.\\n[81] Philip Sun, David Simcha, Dave Dopson, Ruiqi\\nGuo, and Sanjiv Kumar. Soar: Improved quanti-\\nzation for approximate nearest neighbor search.\\nIn Thirty-seventh Conference on Neural Information\\nProcessing Systems, 2023.\\n[82] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre\\nSermanet, Scott Reed, Dragomir Anguelov, Du-\\nmitru Erhan, Vincent Vanhoucke, and Andrew\\nRabinovich. Going deeper with convolutions. In\\nProceedings of the IEEE conference on computer vi-\\nsion and pattern recognition, pages 1–9, 2015.\\n[83] Gergely Szilvasy, Pierre-Emmanuel Mazar ´e, and\\nMatthijs Douze. Vector search with small ra-\\ndiuses, 2024.\\n[84] Romain Tavenard, Herv ´e J ´egou, and Laurent\\nAmsaleg. Balancing clusters to reduce response\\ntime variability in large scale image search. In\\n2011 9th International Workshop on Content-Based\\nMultimedia Indexing (CBMI) , pages 19–24. IEEE,\\n2011.\\n[85] Nandan Thakur, Nils Reimers, Andreas R ¨uckl´e,\\nAbhishek Srivastava, and Iryna Gurevych. BEIR:\\nA heterogeneous benchmark for zero-shot evalu-\\nation of information retrieval models. In Thirty-\\nfifth Conference on Neural Information Processing\\nSystems Datasets and Benchmarks Track (Round 2) ,\\n2021.\\n[86] Bob van Luijt and Micha Verhagen. Bringing\\nsemantic knowledge graph technology to your\\ndata. IEEE Software, 37(2):89–94, 2020.\\n[87] Ash Vardanian. USearch by Unum Cloud, June\\n2022.\\n[88] Jianguo Wang, Xiaomeng Yi, Rentong Guo, Hai\\nJin, Peng Xu, Shengjun Li, Xiangyu Wang, Xi-\\nangzhou Guo, Chengming Li, Xiaohai Xu, et al.\\nMilvus: A purpose-built vector data manage-\\nment system. In Proceedings of the 2021 Inter-\\nnational Conference on Management of Data , pages\\n2614–2627, 2021.\\n[89] Jingdong Wang, Ting Zhang, Nicu Sebe,\\nHeng Tao Shen, et al. A survey on learning\\nto hash. IEEE transactions on pattern analysis and\\nmachine intelligence, 40(4):769–790, 2017.\\n[90] Jun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu\\nChang. Learning to hash for indexing big data -\\na survey. Proc. of the IEEE, 2015.\\n[91] Roger Weber, Hans-J ¨org Schek, and Stephen\\nBlott. A quantitative analysis and performance\\nstudy for similarity-search methods in high-\\ndimensional spaces. In VLDB, volume 98, pages\\n194–205, 1998.\\n[92] Jianyu Zhang, Niklas Nolte, Ranajoy Sadhukhan,\\nBeidi Chen, and L ´eon Bottou. Memory mosaics,\\n2024.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 19, 'page_label': '20'}, page_content='A Appendix\\nThis appendix exposes aspects of Faiss’s implementa-\\ntion. Faiss started in a research environment. As a\\nconsequence, it grew organically as indexing research\\nwas making progress.\\nIn the following, we summarize the guiding prin-\\nciples that keep the library coherent (Appendix A.1);\\nthe structure of the library and its dependencies (Ap-\\npendix A.2); how optimization is performed (Ap-\\npendix A.3); an example of how Faiss internals are ex-\\nposed so that it can be embedded in a vector database\\n(Appendix A.4; and finally, a flowchart that shows\\nhow to choose a Faiss index (Appendix A.5).\\nA.1 Code structure\\nThe core of Faiss is implemented in C++. The guiding\\nprinciples are (1) the code should be as open as pos-\\nsible, so that users can access all the implementation\\ndetails of the indexes; (2) Faiss should be easy to em-\\nbed from external libraries; (3) the core library focuses\\non vector search only.\\nTherefore, all fields of the classes are public (C++\\nstruct). Faiss is a late adopter for C++ standards, so\\nthat it can be used with relatively old compilers (cur-\\nrently C++17).\\nFaiss’s basic data types are concrete (not templates):\\nvectors are always represented as 32-bit floats that are\\nportable and provide a good trade-off between size\\nand accuracy. Similarly, all vector ids are represented\\nwith 64-bit integers. This is often larger than neces-\\nsary for sequential numbering but is widely used for\\ndatabase identifiers.\\nFaiss is modular and includes few dependen-\\ncies, so that linking it from C++ is easy. On\\nthe Python level, callback classes ( ResultHanlder,\\nInvertedLists, IDSelector) can be subclassed and\\nwrapped in SWIG so that they can be provided to\\nFaiss without recompiling Faiss itself.\\nA.2 High-level interface\\nFigure 9 shows the structure of the library. The C++\\ncore library and the GPU add-on have as few depen-\\ndencies as possible: only a BLAS implementation and\\nCUDA itself.\\nIn order to facilitate experimentation, the whole li-\\nbrary is wrapped for Python with numpy. To this\\nend, SWIG 7 exhaustively generates wrappers for all\\nC++ classes, methods and variables. The associ-\\nated Python layer also contains benchmarking code,\\ndataset definitions, driver code. More and more func-\\ntionality is embedded in thecontrib package of Faiss.\\nFaiss also provides a pure C API, which is useful for\\nbindings with programming languages such as Rust\\nor Java.\\nThe Index is presented to the end user as a mono-\\nlithic object, even when it embeds other indexes\\n7https://www.swig.org/\\nIndex implementations\\nIndexIVFPQ, IndexFlat, etc.\\nGPU index implementations\\nGpuIndexIVFPQ, …\\nCUDA library BLAS library \\n(MKL, openblas)\\nUtilities and primitives\\nHeap, ProductQuantizer, Clustering …\\nSWIG wrapper (Python)\\nswigfaiss.swig → swigfaiss.py\\nAdaptor layer \\n__init__.py, class_enhancement.py\\nC wrapper \\nPython contrib library\\n(contrib.datasets, etc.)\\nRust / C#  code\\nC++ code \\ndemos /  tests \\nPython code \\ntests / benchs\\nNumpy library\\nRust wrapper \\nC# wrapper,...\\nFaiss\\nC++/cuda\\n cppcontrib library\\n(SADecodeKernels...)\\nFigure 9: Architecture of the Faiss library. Arrows indicate depen-\\ndencies. Bottom: the library’s dependencies. Top: example of soft-\\nware that depends on Faiss, most notably its extensive test suite.\\nas quantizers, refinement indexes or sharded sub-\\nindexes. Therefore, an index can be duplicated\\nwith clone index and serialized into as a single byte\\nstream using a single function, write index. It also\\ncontains the necessary headers so that it can be read\\nby a generic function, read index.\\nThe index factory. Index objects can be instanti-\\nated explicitly in C++ or Python, but it is more com-\\nmon to build them with the index factory function.\\nThis function takes a string that describes the index\\nstructure and its main parameters. For example, the\\nstring PCA160,IVF20000_HNSW, PQ20x10,RFlat\\ninstantiates an IVF index with KIVF = 20000, where\\nthe coarse quantizer is a HNSW index; then the vec-\\ntors are represented with a PQ20x10 product quan-\\ntizer. The data is preprocessed with a PCA to 160 di-\\nmensions, and the search results are re-ranked with\\na refinement index that performs exact distance com-\\nputations. All the index parameters are set to reason-\\nable defaults, e.g. the PQ encodes the residual of the\\nvectors w.r.t. the coarse quantization centroids. Faiss\\nindexes can be used as vector codecs with functions\\nsa encode, sa decode and sa code size.\\nA.3 Optimization\\nApproach to optimization. Faiss aims at being fea-\\nture complete first. A non-optimal version of all in-\\ndexes is implemented first. Code is optimized only\\nwhen it appears that runtime is important for a cer-\\ntain index. The non-optimized setting is used to con-\\ntrol the correctness of the optimized version.\\nOften, only a subset of data sizes are optimized.\\nFor example, for PQ indexes, only K = 28 and K = 24\\nand d/M ∈ {2, 4, 8, 16, 20} are fully optimized. For\\nIndexLSH search, we only optimized code sizes 4, 8,\\n16 and 20. Fixing these sizes allows to write dedicated\\n“kernels”, i.e., sequences of instructions without ex-\\nplicit loops or tests, that aim to maximize arithmetic\\nthroughput.\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 20, 'page_label': '21'}, page_content='When generic scalar CPU optimizations are ex-\\nhausted, Faiss also optimizes specifically for some\\nhardware platforms.\\nCPU vectorization. Modern CPUs support Single\\nInstruction, Multiple Data (SIMD) operations, specif-\\nically AVX/AVX2/AVX512 for x86 and NEON for\\nARM. Faiss exploits those at three levels.\\nWhen operations are simple enough (e.g. element-\\nwise vector sum), the code is written in a way that the\\ncompiler can vectorize the code by itself, which often\\nboils down to adding restrict keywords to signal\\nthat arrays are not overlapping.\\nThe second level leverages SIMD variables and in-\\nstructions through C++ compiler extensions. Faiss\\nincludes simdlib, a collection of classes intended as\\na layer above the AVX and NEON instruction sets.\\nHowever, much of the SIMD is done specifically for\\none instruction set – most often AVX – because it is\\nmore efficient.\\nThe third level of optimization adapts the data lay-\\nout and algorithms in order to speed up their SIMD\\nimplementation. The 4-bit product and additive quan-\\ntizer implementations are implemented in this way,\\ninspired by the SCANN library [36]: the layout of\\nthe PQ codes for several consecutive vectors is in-\\nterleaved in memory so that a vector permutation\\ncan be used to perform the LUT lookups of (14)\\nin parallel. This is implemented in the FastScan\\nvariants of PQ and AQ indexes ( IndexPQFastScan,\\nIndexIVFResidual QuantizerFastScan, etc.).\\nGPU Faiss. Porting Faiss to the GPU is an involved\\nundertaking due to substantial architectural specifici-\\nties. The implementation of GPU Faiss is detailed in\\n[47], we summarize the GPU implementation chal-\\nlenges therein.\\nModern multi-core CPUs are highly latency opti-\\nmized: they employ an extensive cache hierarchy,\\nbranch prediction, speculative execution and out-of-\\norder code execution to improve serial program exe-\\ncution. In contrast, GPUs have a limited cache hier-\\narchy and omit many of these latency optimizations.\\nThey instead possess a larger number of concurrent\\nthreads of execution (Nvidia’s A100 GPU allows for\\nup to 6,912 warps, each roughly equivalent to a 32-\\nwide vector SIMD CPU thread of execution), a large\\nnumber of floating-point and integer arithmetic func-\\ntional units (A100 has up to 19.5 teraflops per sec-\\nond of fp32 fused-multiply add throughput), and a\\nmassive register set to allow for a high number of\\nlong latency pending instructions in flight (A100 has\\n27 MiB of register memory). They are thus largely\\nthroughput-optimized machines.\\nThe algorithmic techniques used in vector search\\ncan be grouped into three broad categories: dis-\\ntance computation of floating-point or binary vectors\\n(which may have been produced via dequantization\\nfrom a compressed form), table lookups (as seen in\\nPQ distance computations) or scanning (as seen when\\ntraversing IVF lists), and irregular, sequential compu-\\ntations such as linked-list traversal (as used in graph-\\nbased indices) or ranking the k closest vectors.\\nDistance computation is easy on GPUs and read-\\nily exceeds CPU performance, as GPUs are optimized\\nfor matrix-matrix multiplication such as that seen in\\nIndexFlat or IVFFlat. Table lookups and list scan-\\nning can also be made performant on GPUs, as it is\\npossible to stage small tables (as seen in product quan-\\ntization) in shared memory (roughly a user-controlled\\nL1 cache) or register memory and perform lookups in\\nparallel across all warps.\\nSequential table scanning in IVF indices requires\\nloading data from main (global) memory. While main\\nmemory access latency is high, for table scanning we\\nknow in advance what data we wish to access. The\\ndata movement from main memory into registers can\\nthus be pipelined or use double buffering, so we can\\nachieve close to peak possible performance.\\nSelecting the k closest vectors to a query vector by\\nranking distances on the CPU is best implemented\\nwith a min- or max-heap. On the GPU, the sequential\\noperations involved in heap operations would simi-\\nlarly force the GPU into a latency-bound regime. This\\nis the largest challenge for GPU implementation of\\nvector search, as the time needed for the heap im-\\nplementation an order of magnitude greater than all\\nother arithmetic. To handle this, we developed an ef-\\nficient GPU k-selection algorithm [47] that allows for\\nranking candidate vectors in a single pass, operating\\nat a substantial fraction of peak possible performance\\nper memory bandwidth limits. It relies upon heavy\\nusage of the high-speed, large register memory on\\nGPUs, and small-set bitonic sorting via warp shuffles\\nwith buffering techniques.\\nIrregular computations such as walking graph\\nstructures for graph-based indices like HNSW tend\\nto remain in the latency-bound (due to the sequential\\ntraversal) rather than arithmetic throughput or mem-\\nory bandwidth-bound regimes. Here, GPUs are at\\na disadvantage as compared to CPUs, and emerging\\ntechniques such as CAGRA [63] are required to par-\\nallelize otherwise sequential operations with graph\\ntraversal.\\nGPU Faiss implements brute-force GpuIndexFlat\\nas well as the IVF indices GpuIndexIVFFlat,\\nGpuIndexIVF ScalarQuantizer and GpuIndexIVFPQ,\\nwhich are the most useful for large-scale indexing.\\nThe coarse quantizer for the IVF indices can be on\\neither CPU or GPU. The GPU index objects have\\nthe same interface as their CPU counterparts and\\nthe functions index cpu to gpu / index gpu to cpu\\nconvert between them. Multiple GPUs are also\\nsupported. GPU indexes can take inputs and outputs\\nin GPU or CPU memory as input and output, and\\nPython interface can handle Pytorch tensors.\\nAdvanced options for Faiss components and indices.\\nMany Faiss components expose internal parameters\\nto fine-tune the trade-off between metrics: number of\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 21, 'page_label': '22'}, page_content='iterations of k-means, batch sizes for brute-force dis-\\ntance computations, etc. Default parameter values are\\nset to work reasonably well in most cases.\\nMulti-threading. Faiss relies on OpenMP to handle\\nmulti-threading. By default, Faiss switches to multi-\\nthreading processing if it is beneficial, for example,\\nat training and batch addition time. Faiss multi-\\nthreading behavior may be controlled with standard\\nOpenMP environment variables and functions, such\\nas omp set num threads.\\nWhen searching a single vector, Faiss does not\\nspawn multiple threads. However, when batched\\nqueries are provided, Faiss processes them in par-\\nallel, exploiting the effectiveness of the CPU cache\\nand batched linear algebra operations. This is faster\\nthan calling search from multiple threads. Therefore,\\nqueries should be submitted by batches if possible.\\nA.4 Interfacing with external storage\\nFaiss indexes are based on simple storage classes,\\nmainly std::vector to make copy-construction eas-\\nier. The default implementation of IndexIVF is based\\non this storage. However, to give vector database\\ndevelopers more control over the storage of inverted\\nlists, Faiss provides two lower-level APIs.\\nArbitrary inverted lists. The IVF index uses an\\nabstract InvertedLists object as its storage. The\\nobject exposes routines to read one inverted list,\\nadd entries to it and remove entries. The default\\nArrayInvertedLists uses in-memory storage. Al-\\nternatively,OnDiskInverted Listsprovides memory-\\nmapped storage.\\nMore complex implementations can access a key-\\nvalue storage either by storing the entire inverted list\\nas a value, or by utilizing key prefix scan operations\\nlike the one supported by RocksDB to treat multiple\\nkeys prefixed by the same identifier as one inverted\\nlist. To this end, the InvertedLists implementation\\nexposes an InvertedListsIterator and fetches the\\ncodes and ids from the underlying key-value store,\\nwhich usually exposes a similar iterable interface.\\nAdding, updating and removing codes can be dele-\\ngated to the underlying key-value store. We provide\\nan implementation for RocksDB in rocksdb ivf.\\nScanner objects. With the abstraction above, the\\nscanning loop is still controlled by Faiss. If the call-\\ning code needs to control the looping code, then the\\nFaiss IVF index provides an InvertedListScanner\\nobject. The scanner’s state includes the current\\nquery vector and current inverted list. It provides a\\ndistance to code method that, given a code, com-\\nputes the distance from the query to the decom-\\npressed vector. At a slightly higher level, it loops over\\na set of codes and updates a provided result buffer.\\nThis abstraction is useful when the inverted lists are\\nnot stored sequentially or fragmented into sub-lists\\nIVF indexes\\nGraph indexes\\nFlat indexes\\nno\\nyes\\nwill you do a small\\nnumber of queries \\nnq < sqrt(N) ? \\nyessmall database?\\nN < 10k\\nno\\nFlat index \\n(or no indexing at all)\\nyes\\nno\\nDataset is N < 10M\\nand fits in memory\\nseveral times\\nyes\\nno\\ndatabase fixed?\\nallow slow build \\ntime\\nno\\nyesneed exact search\\nresults?\\nNSG,Flat\\nHNSW,Flat\\nno\\nyesN < 1M \\nno\\nyesN < 10M \\nno\\nyesN < 100M \\nyesN < 1B \\nIVF16k\\nflat coarse quantizer\\nIVF64k_HNSW\\nIVF256k_HNSW\\nIVF1M_HNSW\\nyes\\nno\\nhave GPU for \\nindex training?\\nuse 2-level clustering\\ntrain index with GPU\\nyes\\nno\\nM > d IVFx,Flatyes\\nno\\nM > 4d\\nIVFx,SQfp16yes\\nno\\nM > 2d\\nyesM >= d IVFx,SQ8\\nyes\\nno\\nslow build time ok? \\nIVFx,RQM\\nno\\nyessearch speed \\nmore important than\\naccuracy?\\nOPQM/2,IVFx,PQM/2x4fs\\nOPQM,IVFx,PQM\\nstart\\nyes\\nno\\nspeed more important\\nthan accuracy? PQd/2fs,RFlat\\nmemory budget is \\nM + 16 bytes \\nper vector\\nFigure 10: Decision tree to choose a Faiss index. This is for the com-\\nmon case of Euclidean k-nearest neighbor search on CPU. The re-\\nsulting indexes (in bold) are defined by their factory string (see Ap-\\npendix A.2).\\nbecause of metadata filtering [38]. Faiss is used only\\nto perform the coarse quantization and the vector en-\\ncoding.\\nA.5 How to choose an index\\nFigure 10 shows a decision tree for index types, de-\\npending on database size and memory constraints.\\nThe decision tree first focuses on the “hard” mem-\\nory constraint, then on secondary trade-offs, like in-\\ndex construction time vs. accuracy.\\nA.6 API index of the Faiss library\\nAs mentioned in Section 5.3, most indexes are a com-\\nbination of a compression method (like PQ) and a\\nnon-exhaustive search method. The index class names\\nare built as shown in Table 3.\\nFigure 11 shows the most important index classes in\\nFaiss, grouped by broad families. Some combinations\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 22, 'page_label': '23'}, page_content='Table 3: A few combinations of pruning approaches (rows) and\\ncompression methods (columns). In the cells: the corresponding\\nindex implementations.\\nNo encoding PQ encoding scalar quantizer\\nFlat IndexFlat IndexPQ IndexScalarQuantizer\\nIVF IndexIVFFlat IndexIVFPQ IndexIVFScalarQuantizer\\nHNSW IndexHSNWFlat IndexHNSWPQ IndexHNSWScalarQuantizer\\nare omitted for brevity.\\nFaiss index API. In the following, we list the main\\nmethods offered by Faiss indexes, x ∈ Rn×d is a list of\\nn vectors in dimension d represented as the rows of a\\nmatrix, and I ∈ {0, 263 − 1}n is a list of 63-bit ids of\\nsize n.\\n• train(x) perform a training using vectors x to\\nprepare adding vectors of the same data distribu-\\ntion to the index;\\n• add(x) add the vectors x to the index, numbered\\nsequentially;\\n• add with ids(x,I) add the vectors x, identified\\nby the 63-bit ids I;\\n• search(x, k) return the k nearest vectors of each\\nof the query vectors in x;\\n• range search(x, ε) return all vectors within a\\nradius ε of each of the query vectors in x;\\n• remove ids(I) remove vectors with ids I from\\nthe index;\\n• reconstruct batch(I) extract the vectors with\\nids in I. It is called “reconstruct” because for most\\nindex types, the returned vectors will be approx-\\nimations of the original ones.\\nQuantizer objects. Table 4 shows the hierarchy of\\nquantizers. Each quantizer can represent the repro-\\nduction values of all the quantizers below, but is\\nslower to train and to perform assignment with.\\nThe root Quantizer class has the following fields\\nand methods (where x ∈ Rn×d is a list of vectors):\\n• code size size of the codes it produces (in bytes);\\n• train(x) perform a training using vectors x to\\nprepare the quantizer;\\n• compute codes(x) encode a n vectors to an array\\nof size n×(code size);\\n• decode(C) decode the vectors from codes ob-\\ntained with encoder;\\nTable 4: The hierarchy of quantizers. Each quantizer can represent\\nthe set of reproduction values of the quantizers below it.\\nType of quantizer class\\nVector quantizer\\nAdditive quantizer AdditiveQuantizer\\nProduct-additive quantizer ProductAdditiveQuantizer\\nProduct quantizer ProductQuantizer\\nScalar quantizer ScalarQuantizer\\nBinarization\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 23, 'page_label': '24'}, page_content='Zn lattice compression\\ncompressed to binary string \\nQINCo compression \\nRoot of the class hiearchy \\nuncompressed vectors\\ncompressed with scalar quantizer \\ncompressed with a product quantizer\\nadditive quantizer family \\ncompressed vectors stored sequentially\\nIndex built on an inverted ﬁle\\t\\nuncompressed vectors\\t\\nvectors compressed with PQ \\t\\ncompressed with a scalar quantizer\\ncompressed with an additive quantizer\\t\\n(additive quantizer variants)\\nbinary string (with ITQ pre-processing)\\t\\nfast-scan version of the previous \\t\\nHNSW graph-based index\\t\\nuncompressed vectors\\t\\nvectors stored with a scalar quantizer\\t\\nNSG graph-based index \\t\\nFast-scan index (with SIMD based search) \\nAdditive quantizer codes \\nPQ codes  \\nIVF coarse quantizer for an inverted multi-index\\t\\nCoarse quantizer based on additive quantizer\\ntransforms the vectors (PCA, OPQ,...)\\nreorders the output of an approximate index\\ndispatches queries to several identical indexes\\nsplits the database vectors over several indexes\\nremaps sequential ids to arbitrary ids \\nsame, but also allows removing vectors \\t\\nreturns random search results \\t\\nVectors stored sequentially\\nInverted ﬁie indexes\\nGraph indexes\\nFastscan\\nCorase quantizers\\nIndex wrappers\\nPQnx4fs\\t\\nIMI2xb,...\\nRCQnxm,...\\nLSCQnxm,...\\nPCAn,... OPQM,...\\t\\nIDMap,...\\t\\nIDMap2,...\\t\\n...,RFlat\\t\\n\\t\\nHNSWL,Flat\\nHNSWL,SQb\\nNSGL,Flat\\nIVFK,Flat\\t\\nIVFK,PQn\\t\\nIVFK,SQb\\t\\nIVFK,RQnxm\\t\\nIVFK,LSQnxm\\t\\nIVFK,PQnx4fs\\t\\nIVFK,ITQn,SH\\t\\nSQb\\t\\nFlat\\t\\t\\nPQnxm\\t\\nRQnxm\\t\\nLSQnxm\\t\\nPRQpxnxm\\t\\nPLSQpxnxm\\t\\n\\t\\n\\t\\n\\t\\nLSHn\\t\\nZnLattice\\t\\nFigure 11: Hierarchy of the main index classes in CPU Faiss for floating-point vectors. For each class we indicate the corresponding factory\\nstring (when applicable) and a short explanation.\\n24')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is the the docs of total 11 pages, same as pdf pages\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "49ecd013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bfbaad28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-31T08:46:52+00:00', 'author': '', 'keywords': '', 'moddate': '2025-07-31T08:46:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'RPR.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Rikin Pithadia +91-6353865443\\nRoll No : 23035010453 d.pithadia@op.iitg.ac.in\\nB.Sc Hons - Data Science and Artificial Intelligence rikinpithadia98@gmail.com\\nIndian Institute Of Technology, Guwahati github.com/rikin-2911\\nlinkedin.com/in/rikin-pithadia\\nEducation\\nDegree/Certificate Institute/Board CGPA/Percentage Year\\nB.Sc.(Hons) in Data Science and AI Indian Institute of Technology,\\nGuwahati\\n7.42 (Current) 2023 - Present\\nB.Tech in Mechanical Engineering Government Engineering College,\\nGandhinagar\\n7.79 (Current) 2023 - Present\\nMinor in Internet of Things (IoT) Government Engineering College,\\nGandhinagar\\n8.00 (Current) 2024 - Present\\nProjects\\n• 1. Natural Image Classification | CNNs, ResNets, PyTorch, Intel Images May 2025 - June 2025\\nImage Classification using Deep Learning github.com/rikin-2911/ResNet9\\n– Developed a customResNet9 CNN modelusing PyTorchto classify natural scene images into6 categories, trained\\non 14,000+ images and obtained90.27% accuracyon validation set(3,000+ images) using advancedimage augmen-\\ntation techniques(Resize, Random-Crop, Random-Rotation, Normalization)\\n– Automated dataset retrieval viaKaggle APIand deployed the model with aStreamlit web appfor real-time image\\nclassification and can be utilized inTerrain Assessment Systems.\\n• 2. Generative AI for Text Generation | LSTM, Deepseek-R1, Streamlit Feb 2025 - March 2025\\nHack the Spring - Technoverse, GEC Gandhinagar github.com/rikin-2911/TextGenAI\\n– Built a Generative AI system using custom LSTM model for story and dialogue generation, and integrated\\nDeepseek-R1 LLM via Ollama APIfor a generalizedchatbot, ranked intop 1%at GEC Gandhinagar Hackathon.\\n– Deployed both models through an interactive Streamlit web app. Demonstratedend-to-end development, from\\ndata preparation to UI deployment, highlighting model versatility and user-centric design.\\n• 3. IoT-Based Smart Home Energy Prediction | XGBoost, IoT Data Jan 2025 - Feb 2025\\nIntegration of AI/ML Systems in IoT github.com/rikin-2911/IoTEnergyPred\\n– Designed anML pipelineto predict energy usage in IoT-enabled smart homes usingXGBoost, and achievedRMSE\\nscore of 0.158on test data.\\n– Achieved 97.76% accuracy on test data throughdata preprocessing (imputation, scaling, encoding)and handled\\nclass imbalance effectively.\\n– Enabled energy pattern insightsvia detailedEDA and visualizations usingSeaborn and Matplotlib.\\n• 4. Anomaly Detection in Credit Card Transactions | Scikit-learn Jan 2025 - Feb 2025\\nFraud Detection using Machine Learning github.com/rikin-2911/CCFraudDetect\\n– Built aRandom Forest modelto detect fraudulent credit card transactions with98% test accuracy. Utilized data\\nvisualization (Matplotlib, Seaborn and Plotly Express)for exploratory analysis and pattern identification.\\nTechnical Skills\\n• Programming: C, Python*, R, Java\\n• Python Frameworks:PyTorch*, TensorFlow, Keras\\n• Python Tools:Numpy, Pandas, Scikit-learn*, Matplotlib\\n• Generative AI:LangChain*, LLMs, Prompt\\nEngineering*, RAG\\n• Machine Learning:Supervised and Unsupervised\\nLearning, Algorithms*, Feature Engineering\\n• Deep Learning: Computer Vision, NLP, CNN, RNN,\\nLSTM, Transformers\\n• Deployment: FastAPI*, Docker, Streamlit*, Git,\\nGitHub* * Elementary proficiency\\nRelevant Coursework\\n• Mathematics: Probability and Statistics, Calculus, Linear Algebra, Optimization, Statistical Inferencing\\n• AI/ML: Machine Learning Fundamentals, Time Series Analysis, Recommender Systems\\n• Electrical and Electronics: Signals and Systems\\nCertifications and Achievements\\n• Team Leader - Placement Fair Volunteer Section, GEC-Gandhinagar 2025\\n• LangChain Essentials for Generative AI, LangChain Web 2025\\n• Trustworthy Generative AI, Coursera 2024\\n• Machine Learning with Python, Jovian 2024\\n• Business Writing - Effective Communication, Coursera 2024')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfed6150",
   "metadata": {},
   "source": [
    ">> Step2 Indexing (Chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fdf8d962",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "087e70cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff19a3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "358035f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-12T02:08:57+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-12T02:08:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'F.pdf', 'total_pages': 24, 'page': 0, 'page_label': '1'}, page_content='We benchmark key features of the library and discuss\\na few selected applications to highlight its broad ap-\\nplicability.\\n1 Introduction\\nThe emergence of deep learning has induced a shift in\\nhow complex data is stored and searched, noticeably\\nby the development of embeddings. Embeddings are\\nvector representations, typically produced by a neu-\\nral network, that map (embed) the input media item\\ninto a vector space, where the locality encodes the se-\\nmantics of the input. Embeddings are extracted from\\nvarious forms of media: words [59, 10], text [24, 40],\\nimages [15, 69], users and items for recommenda-\\ntion [65]. They can even encode object relations, for\\ninstance multi-modal text-image or text-audio rela-\\ntions [31, 70].\\nEmbeddings are employed as an intermediate rep-\\nresentation for further processing, e.g. self-supervised\\nimage embeddings are input to shallow supervised\\nimage classifiers [14, 15]. They are also leveraged as\\na pretext task for self-supervision [18]. In fact, embed-')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a604beb5",
   "metadata": {},
   "source": [
    ">> Step 3 Indexing (Embeddings and Vector Stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1ca2ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we setup our LLM for embedding task\n",
    "embedding_llm = HuggingFaceEndpointEmbeddings(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "# Now we create a vector store for storing our embeddings/vectors\n",
    "vector_store = FAISS.from_documents(chunks, embedding_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828437fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'e495f8c6-eb2f-4b4d-ace2-02dfc2fbfee3',\n",
       " 1: '080840da-8c31-427d-9036-cb8e276c3d5f',\n",
       " 2: 'c7eca0ef-8a14-4098-a419-4007b7535821',\n",
       " 3: 'b1fd5697-3194-4d35-87ea-0ddd28641f26',\n",
       " 4: '7a15314d-9415-4942-b154-860353b92bbd'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For viewing the particular id's  ($0 id's for 40 chunks)\n",
    "vector_store.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1948f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.get_by_ids(['99caca0e-4324-469b-9e06-a9367afd9e0d'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b535c",
   "metadata": {},
   "source": [
    ">> Step 4 Reteiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3e5454d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for retreiving, the process is straight forward  beacuse our data is in the vector store\n",
    "# Therefore we use vector store as our retriever \n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\":4}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "50030347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEndpointEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002D6A7438E90>, search_type='mmr', search_kwargs={'k': 4})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14086ab8",
   "metadata": {},
   "source": [
    ">> Step 5 Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c0ddb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up llm for text generation\n",
    "qwen_llm = HuggingFaceEndpoint(\n",
    "    #repo_id=\"Qwen/Qwen3-235B-A22B-Instruct-2507\",\n",
    "    #repo_id=\"openai/gpt-oss-120b\",   # For now this is good, but not for last step\n",
    "    #repo_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    #repo_id=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    repo_id=\"google/gemma-2-2b-it\",\n",
    "    #repo_id=\"deepseek-ai/DeepSeek-R1-0528\",\n",
    "    #repo_id=\"perplexity-ai/r1-1776-distill-llama-70b\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=500,\n",
    "    huggingfacehub_api_token=os.getenv(\"HF_API_KEY\")\n",
    ")\n",
    "\n",
    "llm = ChatHuggingFace(llm=qwen_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "733b7e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing prompt for LLM input\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "        You are a personalized research Assitant for providing meaningful answers.\n",
    "        Answer in simple meaning and detailed explanation of the topic for user understanding.\n",
    "        Answer ONLY from the provided transcript context.\n",
    "        If the context is insufficient, just say you don't know,\n",
    "            \n",
    "        {context}\n",
    "        Question: {question}\n",
    "    \"\"\",\n",
    "    input_variables=['context', 'question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca93f150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} template=\"\\n        You are a personalized research Assitant for providing meaningful answers.\\n        Answer in simple meaning and detailed explanation of the topic for user understanding.\\n        Answer ONLY from the provided transcript context.\\n        If the context is insufficient, just say you don't know,\\n            \\n        {context}\\n        Question: {question}\\n    \"\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f90cc911",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is skills\"\n",
    "retrived_docs = retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d49a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='7a15314d-9415-4942-b154-860353b92bbd', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-31T08:46:52+00:00', 'author': '', 'keywords': '', 'moddate': '2025-07-31T08:46:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'RPR.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='• Electrical and Electronics: Signals and Systems\\nCertifications and Achievements\\n• Team Leader - Placement Fair Volunteer Section, GEC-Gandhinagar 2025\\n• LangChain Essentials for Generative AI, LangChain Web 2025\\n• Trustworthy Generative AI, Coursera 2024\\n• Machine Learning with Python, Jovian 2024\\n• Business Writing - Effective Communication, Coursera 2024'),\n",
       " Document(id='080840da-8c31-427d-9036-cb8e276c3d5f', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-31T08:46:52+00:00', 'author': '', 'keywords': '', 'moddate': '2025-07-31T08:46:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'RPR.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='– Developed a customResNet9 CNN modelusing PyTorchto classify natural scene images into6 categories, trained\\non 14,000+ images and obtained90.27% accuracyon validation set(3,000+ images) using advancedimage augmen-\\ntation techniques(Resize, Random-Crop, Random-Rotation, Normalization)\\n– Automated dataset retrieval viaKaggle APIand deployed the model with aStreamlit web appfor real-time image\\nclassification and can be utilized inTerrain Assessment Systems.\\n• 2. Generative AI for Text Generation | LSTM, Deepseek-R1, Streamlit Feb 2025 - March 2025\\nHack the Spring - Technoverse, GEC Gandhinagar github.com/rikin-2911/TextGenAI\\n– Built a Generative AI system using custom LSTM model for story and dialogue generation, and integrated\\nDeepseek-R1 LLM via Ollama APIfor a generalizedchatbot, ranked intop 1%at GEC Gandhinagar Hackathon.\\n– Deployed both models through an interactive Streamlit web app. Demonstratedend-to-end development, from'),\n",
       " Document(id='b1fd5697-3194-4d35-87ea-0ddd28641f26', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-31T08:46:52+00:00', 'author': '', 'keywords': '', 'moddate': '2025-07-31T08:46:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'RPR.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='– Built aRandom Forest modelto detect fraudulent credit card transactions with98% test accuracy. Utilized data\\nvisualization (Matplotlib, Seaborn and Plotly Express)for exploratory analysis and pattern identification.\\nTechnical Skills\\n• Programming: C, Python*, R, Java\\n• Python Frameworks:PyTorch*, TensorFlow, Keras\\n• Python Tools:Numpy, Pandas, Scikit-learn*, Matplotlib\\n• Generative AI:LangChain*, LLMs, Prompt\\nEngineering*, RAG\\n• Machine Learning:Supervised and Unsupervised\\nLearning, Algorithms*, Feature Engineering\\n• Deep Learning: Computer Vision, NLP, CNN, RNN,\\nLSTM, Transformers\\n• Deployment: FastAPI*, Docker, Streamlit*, Git,\\nGitHub* * Elementary proficiency\\nRelevant Coursework\\n• Mathematics: Probability and Statistics, Calculus, Linear Algebra, Optimization, Statistical Inferencing\\n• AI/ML: Machine Learning Fundamentals, Time Series Analysis, Recommender Systems\\n• Electrical and Electronics: Signals and Systems\\nCertifications and Achievements'),\n",
       " Document(id='e495f8c6-eb2f-4b4d-ace2-02dfc2fbfee3', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-31T08:46:52+00:00', 'author': '', 'keywords': '', 'moddate': '2025-07-31T08:46:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'RPR.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Rikin Pithadia +91-6353865443\\nRoll No : 23035010453 d.pithadia@op.iitg.ac.in\\nB.Sc Hons - Data Science and Artificial Intelligence rikinpithadia98@gmail.com\\nIndian Institute Of Technology, Guwahati github.com/rikin-2911\\nlinkedin.com/in/rikin-pithadia\\nEducation\\nDegree/Certificate Institute/Board CGPA/Percentage Year\\nB.Sc.(Hons) in Data Science and AI Indian Institute of Technology,\\nGuwahati\\n7.42 (Current) 2023 - Present\\nB.Tech in Mechanical Engineering Government Engineering College,\\nGandhinagar\\n7.79 (Current) 2023 - Present\\nMinor in Internet of Things (IoT) Government Engineering College,\\nGandhinagar\\n8.00 (Current) 2024 - Present\\nProjects\\n• 1. Natural Image Classification | CNNs, ResNets, PyTorch, Intel Images May 2025 - June 2025\\nImage Classification using Deep Learning github.com/rikin-2911/ResNet9\\n– Developed a customResNet9 CNN modelusing PyTorchto classify natural scene images into6 categories, trained')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrived_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fdef828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = \"\\n\\n\".join(doc.page_content for doc in retrived_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12b1c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'• Electrical and Electronics: Signals and Systems\\nCertifications and Achievements\\n• Team Leader - Placement Fair Volunteer Section, GEC-Gandhinagar 2025\\n• LangChain Essentials for Generative AI, LangChain Web 2025\\n• Trustworthy Generative AI, Coursera 2024\\n• Machine Learning with Python, Jovian 2024\\n• Business Writing - Effective Communication, Coursera 2024\\n\\n– Developed a customResNet9 CNN modelusing PyTorchto classify natural scene images into6 categories, trained\\non 14,000+ images and obtained90.27% accuracyon validation set(3,000+ images) using advancedimage augmen-\\ntation techniques(Resize, Random-Crop, Random-Rotation, Normalization)\\n– Automated dataset retrieval viaKaggle APIand deployed the model with aStreamlit web appfor real-time image\\nclassification and can be utilized inTerrain Assessment Systems.\\n• 2. Generative AI for Text Generation | LSTM, Deepseek-R1, Streamlit Feb 2025 - March 2025\\nHack the Spring - Technoverse, GEC Gandhinagar github.com/rikin-2911/TextGenAI\\n– Built a Generative AI system using custom LSTM model for story and dialogue generation, and integrated\\nDeepseek-R1 LLM via Ollama APIfor a generalizedchatbot, ranked intop 1%at GEC Gandhinagar Hackathon.\\n– Deployed both models through an interactive Streamlit web app. Demonstratedend-to-end development, from\\n\\n– Built aRandom Forest modelto detect fraudulent credit card transactions with98% test accuracy. Utilized data\\nvisualization (Matplotlib, Seaborn and Plotly Express)for exploratory analysis and pattern identification.\\nTechnical Skills\\n• Programming: C, Python*, R, Java\\n• Python Frameworks:PyTorch*, TensorFlow, Keras\\n• Python Tools:Numpy, Pandas, Scikit-learn*, Matplotlib\\n• Generative AI:LangChain*, LLMs, Prompt\\nEngineering*, RAG\\n• Machine Learning:Supervised and Unsupervised\\nLearning, Algorithms*, Feature Engineering\\n• Deep Learning: Computer Vision, NLP, CNN, RNN,\\nLSTM, Transformers\\n• Deployment: FastAPI*, Docker, Streamlit*, Git,\\nGitHub* * Elementary proficiency\\nRelevant Coursework\\n• Mathematics: Probability and Statistics, Calculus, Linear Algebra, Optimization, Statistical Inferencing\\n• AI/ML: Machine Learning Fundamentals, Time Series Analysis, Recommender Systems\\n• Electrical and Electronics: Signals and Systems\\nCertifications and Achievements\\n\\nRikin Pithadia +91-6353865443\\nRoll No : 23035010453 d.pithadia@op.iitg.ac.in\\nB.Sc Hons - Data Science and Artificial Intelligence rikinpithadia98@gmail.com\\nIndian Institute Of Technology, Guwahati github.com/rikin-2911\\nlinkedin.com/in/rikin-pithadia\\nEducation\\nDegree/Certificate Institute/Board CGPA/Percentage Year\\nB.Sc.(Hons) in Data Science and AI Indian Institute of Technology,\\nGuwahati\\n7.42 (Current) 2023 - Present\\nB.Tech in Mechanical Engineering Government Engineering College,\\nGandhinagar\\n7.79 (Current) 2023 - Present\\nMinor in Internet of Things (IoT) Government Engineering College,\\nGandhinagar\\n8.00 (Current) 2024 - Present\\nProjects\\n• 1. Natural Image Classification | CNNs, ResNets, PyTorch, Intel Images May 2025 - June 2025\\nImage Classification using Deep Learning github.com/rikin-2911/ResNet9\\n– Developed a customResNet9 CNN modelusing PyTorchto classify natural scene images into6 categories, trained'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6426d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = prompt.invoke({'context':context_text, 'question':question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88c22f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=\"\\n        You are a personalized research Assitant for providing meaningful answers.\\n        Answer in simple meaning and detailed explanation of the topic for user understanding.\\n        Answer ONLY from the provided transcript context.\\n        If the context is insufficient, just say you don't know,\\n            \\n        • Electrical and Electronics: Signals and Systems\\nCertifications and Achievements\\n• Team Leader - Placement Fair Volunteer Section, GEC-Gandhinagar 2025\\n• LangChain Essentials for Generative AI, LangChain Web 2025\\n• Trustworthy Generative AI, Coursera 2024\\n• Machine Learning with Python, Jovian 2024\\n• Business Writing - Effective Communication, Coursera 2024\\n\\n– Developed a customResNet9 CNN modelusing PyTorchto classify natural scene images into6 categories, trained\\non 14,000+ images and obtained90.27% accuracyon validation set(3,000+ images) using advancedimage augmen-\\ntation techniques(Resize, Random-Crop, Random-Rotation, Normalization)\\n– Automated dataset retrieval viaKaggle APIand deployed the model with aStreamlit web appfor real-time image\\nclassification and can be utilized inTerrain Assessment Systems.\\n• 2. Generative AI for Text Generation | LSTM, Deepseek-R1, Streamlit Feb 2025 - March 2025\\nHack the Spring - Technoverse, GEC Gandhinagar github.com/rikin-2911/TextGenAI\\n– Built a Generative AI system using custom LSTM model for story and dialogue generation, and integrated\\nDeepseek-R1 LLM via Ollama APIfor a generalizedchatbot, ranked intop 1%at GEC Gandhinagar Hackathon.\\n– Deployed both models through an interactive Streamlit web app. Demonstratedend-to-end development, from\\n\\n– Built aRandom Forest modelto detect fraudulent credit card transactions with98% test accuracy. Utilized data\\nvisualization (Matplotlib, Seaborn and Plotly Express)for exploratory analysis and pattern identification.\\nTechnical Skills\\n• Programming: C, Python*, R, Java\\n• Python Frameworks:PyTorch*, TensorFlow, Keras\\n• Python Tools:Numpy, Pandas, Scikit-learn*, Matplotlib\\n• Generative AI:LangChain*, LLMs, Prompt\\nEngineering*, RAG\\n• Machine Learning:Supervised and Unsupervised\\nLearning, Algorithms*, Feature Engineering\\n• Deep Learning: Computer Vision, NLP, CNN, RNN,\\nLSTM, Transformers\\n• Deployment: FastAPI*, Docker, Streamlit*, Git,\\nGitHub* * Elementary proficiency\\nRelevant Coursework\\n• Mathematics: Probability and Statistics, Calculus, Linear Algebra, Optimization, Statistical Inferencing\\n• AI/ML: Machine Learning Fundamentals, Time Series Analysis, Recommender Systems\\n• Electrical and Electronics: Signals and Systems\\nCertifications and Achievements\\n\\nRikin Pithadia +91-6353865443\\nRoll No : 23035010453 d.pithadia@op.iitg.ac.in\\nB.Sc Hons - Data Science and Artificial Intelligence rikinpithadia98@gmail.com\\nIndian Institute Of Technology, Guwahati github.com/rikin-2911\\nlinkedin.com/in/rikin-pithadia\\nEducation\\nDegree/Certificate Institute/Board CGPA/Percentage Year\\nB.Sc.(Hons) in Data Science and AI Indian Institute of Technology,\\nGuwahati\\n7.42 (Current) 2023 - Present\\nB.Tech in Mechanical Engineering Government Engineering College,\\nGandhinagar\\n7.79 (Current) 2023 - Present\\nMinor in Internet of Things (IoT) Government Engineering College,\\nGandhinagar\\n8.00 (Current) 2024 - Present\\nProjects\\n• 1. Natural Image Classification | CNNs, ResNets, PyTorch, Intel Images May 2025 - June 2025\\nImage Classification using Deep Learning github.com/rikin-2911/ResNet9\\n– Developed a customResNet9 CNN modelusing PyTorchto classify natural scene images into6 categories, trained\\n        Question: what is skills\\n    \"\n"
     ]
    }
   ],
   "source": [
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493c86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"\\n        You are a personalized research Assitant for providing meaningful answers.\\n        Answer in simple meaning and detailed explanation of the topic for user understanding.\\n        Answer ONLY from the provided transcript context.\\n        If the context is insufficient, just say you don't know,\\n            \\n        • Electrical and Electronics: Signals and Systems\\nCertifications and Achievements\\n• Team Leader - Placement Fair Volunteer Section, GEC-Gandhinagar 2025\\n• LangChain Essentials for Generative AI, LangChain Web 2025\\n• Trustworthy Generative AI, Coursera 2024\\n• Machine Learning with Python, Jovian 2024\\n• Business Writing - Effective Communication, Coursera 2024\\n\\n– Developed a customResNet9 CNN modelusing PyTorchto classify natural scene images into6 categories, trained\\non 14,000+ images and obtained90.27% accuracyon validation set(3,000+ images) using advancedimage augmen-\\ntation techniques(Resize, Random-Crop, Random-Rotation, Normalization)\\n– Automated dataset retrieval viaKaggle APIand deployed the model with aStreamlit web appfor real-time image\\nclassification and can be utilized inTerrain Assessment Systems.\\n• 2. Generative AI for Text Generation | LSTM, Deepseek-R1, Streamlit Feb 2025 - March 2025\\nHack the Spring - Technoverse, GEC Gandhinagar github.com/rikin-2911/TextGenAI\\n– Built a Generative AI system using custom LSTM model for story and dialogue generation, and integrated\\nDeepseek-R1 LLM via Ollama APIfor a generalizedchatbot, ranked intop 1%at GEC Gandhinagar Hackathon.\\n– Deployed both models through an interactive Streamlit web app. Demonstratedend-to-end development, from\\n\\n– Built aRandom Forest modelto detect fraudulent credit card transactions with98% test accuracy. Utilized data\\nvisualization (Matplotlib, Seaborn and Plotly Express)for exploratory analysis and pattern identification.\\nTechnical Skills\\n• Programming: C, Python*, R, Java\\n• Python Frameworks:PyTorch*, TensorFlow, Keras\\n• Python Tools:Numpy, Pandas, Scikit-learn*, Matplotlib\\n• Generative AI:LangChain*, LLMs, Prompt\\nEngineering*, RAG\\n• Machine Learning:Supervised and Unsupervised\\nLearning, Algorithms*, Feature Engineering\\n• Deep Learning: Computer Vision, NLP, CNN, RNN,\\nLSTM, Transformers\\n• Deployment: FastAPI*, Docker, Streamlit*, Git,\\nGitHub* * Elementary proficiency\\nRelevant Coursework\\n• Mathematics: Probability and Statistics, Calculus, Linear Algebra, Optimization, Statistical Inferencing\\n• AI/ML: Machine Learning Fundamentals, Time Series Analysis, Recommender Systems\\n• Electrical and Electronics: Signals and Systems\\nCertifications and Achievements\\n\\nRikin Pithadia +91-6353865443\\nRoll No : 23035010453 d.pithadia@op.iitg.ac.in\\nB.Sc Hons - Data Science and Artificial Intelligence rikinpithadia98@gmail.com\\nIndian Institute Of Technology, Guwahati github.com/rikin-2911\\nlinkedin.com/in/rikin-pithadia\\nEducation\\nDegree/Certificate Institute/Board CGPA/Percentage Year\\nB.Sc.(Hons) in Data Science and AI Indian Institute of Technology,\\nGuwahati\\n7.42 (Current) 2023 - Present\\nB.Tech in Mechanical Engineering Government Engineering College,\\nGandhinagar\\n7.79 (Current) 2023 - Present\\nMinor in Internet of Things (IoT) Government Engineering College,\\nGandhinagar\\n8.00 (Current) 2024 - Present\\nProjects\\n• 1. Natural Image Classification | CNNs, ResNets, PyTorch, Intel Images May 2025 - June 2025\\nImage Classification using Deep Learning github.com/rikin-2911/ResNet9\\n– Developed a customResNet9 CNN modelusing PyTorchto classify natural scene images into6 categories, trained\\n        Question: what is skills\\n    \")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fcabbb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = llm.invoke(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a3909952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is about **Faiss**, a powerful index library used in machine learning tasks. To understand skills, let's break down the table of contents and look at some key concepts:\n",
      "\n",
      "**Skills:**  Are generally abilities, knowledge, and traits that help in performing tasks and accomplishing goals. \n",
      " \n",
      "**Faiss:**  This name appears in a scientific context, referring to an open-source library. What this library is used for is crucial to understanding its \"skills\"\n",
      "\n",
      "Here's the key takeaway to understand the concept of skills from this document:\n",
      "\n",
      "* **Artificial Intelligence (AI) and Computer Vision:** This document focuses on how AI researchers are processing large amounts of data (pictures, video), using a library made for this task to build great models.\n",
      "* **Machine Learning (ML):** This is the process where computers are trained to learn from data to model various things. AI that relates to pattern recognition in images or videos, FAISS can be a versatile tool to set up machine learning models.\n",
      "\n",
      "\n",
      "In simple terms, Faiss helps scientists train AI algorithms by providing tools to organize and work with large amounts of data. This data analysis process allows building robust models to solve complex problems.\n",
      "\n",
      "Let me know if you'd like more information about one of these concepts.\n"
     ]
    }
   ],
   "source": [
    "print(results.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c8f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = \"\\n\\n\".join(doc.page_content for doc in chunks)\n",
    "summary_prompt=PromptTemplate(\n",
    "            template=\"\"\"\n",
    "                    Summarize the following documents:- \\n {context_text}\n",
    "            \"\"\",\n",
    "            input_variables=['context_text']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93175536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context_text'] input_types={} partial_variables={} template='\\n                    Summarize the following documents:- \\n {context_text}\\n            '\n"
     ]
    }
   ],
   "source": [
    "print(summary_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
